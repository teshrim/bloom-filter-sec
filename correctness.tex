\section{Correctness Against Adaptive Adversaries}
As highlighted by Naor and Yogev, Equation~(\ref{eqn:correctness}) is a non-adaptive version of correctness;
we are interested in defining an adaptive (i.e., adversarial) notion
of correctness.   Figure~\ref{fig:correctness} describes a security
experiment that captures such a notion.

First, a set~$S$ is chosen according to some
distribution~$\distr{\calS}{}$ over the collection of multisets
$\mathcal{S}$ associated to~$\Pi=(\Rep,\Qry)$, and then
$\Rep(S)$ is run to generate $(M,\aux)$.
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\calS$ and $\aux$~as input; it is
provided an oracle~$\TestOracle$ that, on input a query~$q \in \calQ$,
computes (and returns) the result of $\Qry(M,\aux,q)$, and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.

The notion is lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).



\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpCorrect{\setprim,\distr{\calS}{},r}{A}$}\\
$S \getsr \distr{\calS}{}$\\
$\mathrm{err}\gets 0$\\
$(M,\aux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(S,\aux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\aux,q)$\\
if $a \neq q(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$a$
}
}
\caption{Correctness of~$\Pi$ against (adaptive) adversary~$A$, when
  the target multiset~$S$ is sampled from~$\calS$ according to distribution~$\distr{\calS}{}$.} 
\label{fig:correctness}
\end{figure}

In this experiment we track the time-complexity (relative to some
implied model of computation) and query-complexity (i.e., number of
queries) of the adversary~$A$.  
We define the advantage of adversary~$A$ in the correctness experiment as 
$\AdvCorrect{\Pi,\distr{\calS}{},r}{A} = \Prob{\ExpCorrect{\Pi,\distr{\calS}{}, r,}{A}=1}$.
Overloading notation, we write $\AdvCorrect{\Pi,\distr{\calS}{},r}{t,q}$ for the maximum over all~$t$-time adversaries that ask at most~$q$ queries. Roughly, we say that a
set-representation~$\Pi$ is $(t,q,r,\epsilon')$-correct if $\AdvCorrect{\Pi,\distr{\calS}{},r}{t,q} \leq \epsilon'$.  

\def\bin{{\sf Bin}}
\heading{Observations on trivial attacks. } 
Let~$\Pi$ be a scheme that is $\epsilon$-correct, and consider an attack that 
asks random queries $q_1,q_2,\ldots,q_T$, i.e., uniform samples from~$\calQ$.
%The expected number of random queries needed to
%find a error is $1/\epsilon$ tries; thus, an attacker making $O(t/\epsilon)$ queries succeeds with constant probability (almost always).
%Actually, it seems worthwhile to formalize this. 
Let $X_i$ be a random variable indicating the event $\Qry(M,\aux,q_i)\neq q_i(S)$.  
The $\epsilon$-correctness of~$\Pi$ guarantees that the probability that $X_i=1$ is at most~$\epsilon$, 
and it is tempting to model the~$X_i$ as i.i.d. Bernoulli trials with success probability~$\epsilon$.
However, a moment's reflection shows that this is not the case.  Concretely, consider the following scheme.  Fix $|S|=n$ for all $S \in \calS$, where $n \ll |\univ|$.  For the queries, let $\calQ = \{q_x\colon \calS \to \bits \,|\, \forall x \in \univ\}$ where $\forall S \in \calS$ the predicate $q_x(S)=1 \Leftrightarrow x \in S$.  Let $\Rep(S)$ pick a random element~$s \in S$, and return $M = S \setminus \{s\}$ and $\aux = \emptystring$.  To respond to queries, let~$\Qry(M,\aux,q_x)=1$ iff $x \in M$.  It is easy to see that this scheme is $1/n$-correct; for any fixed~$S$, there are exactly~$n$ queries~$q_x$ that have probability~$1/n$  (over the coins of~$\Rep$) of causing a correctness error.  Moreover, once the coins of~$\Rep$ are fixed, there is exactly \emph{one} query that will cause an error.  Thus for random queries from~$Q$, for all~$i\in[T]$ we have $\Prob{X_i=1}=1/|\univ| \ll 1/n = \epsilon$.

This example is contrived, but the point is that $\epsilon$-correctness is measured over the coins of~$\Rep$, for a fixed~$S$ and query~$q$; once the coins of~$\Rep$ are fixed, it is not straightforward how to connect~$\epsilon$ to events that depend on other sources of randomness. \tsnote{Not sure if there's more to say in this section, or if what's here is already too much.}


%
%And if the experiment requires the adversary to induce~$r$ errors, the attack wins iff
%$N \stackrel{{\rm def}}{=} \sum_{i=1}^T X_i \geq r$.  Modelling the $X_i$ as independent Bernoulli trials with
%probability~$\epsilon$ gives $N$ a binomial distribution.  \fixme{Something feels wrong with this...}  
%Writing $\bin(T, \epsilon; r)$ for the probability that $N \geq r$,
%we conclude that the \emph{best} correctness we can hope for against adversaries making~$T$ queries is  $\bin(T,\epsilon; t))$. %\jnote{Need to find different terms to use for a query $q \in \mathcal{Q}$
%and a query that the attacker makes to its oracle.}
%\tsnote{Viewed another way, we could let~$T$ be a negative binomial RV, and ask how many queries are needed (w.h.p) for $N \geq t$.} 
%
%\jnote{One option is to define the adversary's \emph{advantage} as the difference between
%its success probability and the success probability of the above,
%trivial attack. Just a thought.}
%\tsnote{Or, perhaps by recasting security
%  as distinguishing between $\Qry(M,\aux,\cdot)$ and an oracle that
%  returns 0,1 according to Bernoulli with paramter~$\epsilon$.}
%\fixme{Not clear this attack is generically applicable enough to serve as a reference/baseline.  (See email discusssion 7/23.)  Might be worth pointing out that this intuition, between non-adaptive error-probability and the expected number of queries to find an error, can be quite misleading.}

\heading{Comparison to Naor-Yogev definition.}
The style of the Naor-Yogev (NY) definition is slightly different. \jnote{Even if we ignore
all the other issues, chief among them the inconsistent way it treats the hash functions.}
Lifting their definition to our setting, the experiment is similar but the attacker
succeeds only if it outputs a single query $q$ for which $\Qry( M,
\aux, q) \neq q(S)$  \emph{subject to the restriction that it did not query $q$ to its $\Qry$ oracle}.
It will be interesting to understand formally the relation between our definition and theirs.
Here are some observations:
\begin{enumerate}
\item Say a scheme is $\epsilon_{NY}$-secure under the NY definition.
Then we claim it is $(t,q, r, q\epsilon_{NY}/r)$-correct. To see
this, assume not. Then there is a $t$-time attacker~$A$ that, with
probability at least $\epsilon'=q\epsilon_{NY}/r$,
issues~$q$ Test-queries of which at least~$r$ cause errors. If we simply run~$A$ and then choose a random Test-query to
output, we succeed with probability at least $\epsilon' \cdot r/q$. It is unclear whether this is tight.

\item There is a contrived scheme that is $(t, q, 2, 0)$-correct but not NY-secure at all; the basic idea is
to have a set-representation that always has \emph{exactly one} false positive that is easy to find
given the public information.

\item If a scheme is $(t,q,1,\epsilon)$-correct, then it is also $\epsilon$-secure under the NY definition,
at least for attackers making at most $q-1$ queries.
This is immediate.
\end{enumerate}




\if{0}
\heading{FP-security of hash-based filters.}
%\tsnote{NY give it in the private hash setting (sort of); but, intuitively, it should be hard to find false-positives even if you know that MD5 or SHA-x are the hash functions.  Not totally sure how to capture this intuition nicely.}
%
%\jnote{Not sure I agree. First of all, if $\epsilon$ is not negligible then a random element is a false positive with noticeable probability. Beyond that, I think that intuitively it makes sense that if you want false positives to be hard to find then you need to make the hash functions secret.}
%
%\tsnote{I don't think it makes sense to talk about asymptotics or negligibility, for exactly the reason you state. I think the key point here is: what is ``hard''? The designer decides what FP rate is acceptable for a given application, a given set of efficiency constraints, the side-effects of a FP, etc.  For a fixed FP target, she optimizes the filter size and/or the number of hash functions for her setting. Whether the target FP rate is 0.01 or $2^{-80}$, we should be able to say (in the ROM, at least) that it will take this-many hash computations, and this many on-line set-membership queries, to find the first FP.  In the standard model, with public hash functions, it is clearly more difficult to make formal claims.  Even so, in practice I expect it would be quite expensive to find a string~$x$ such that $(h_1(x),h_2(x),\ldots,h_k(x))$ falls in a given set of tuples when $h_i(x)=\mathrm{truncate}(\mathrm{SHA256}(\langle i \rangle \concat x), m)$, say. (Even more so if my setting admits relatively expensive hash computations, e.g. SCRYPT.) BTW, your suggestion to have a FP notion with secret~$\mathcal{S}$ is interesting here. }
%
The traditional notion of soundness for a Bloom filter captures the difficulty of finding false positives when the attacker is non-adaptive. Following NY, we consider soundness against adaptive adversaries. But we further generalize to allow for settings in which the attacker may have some a priori knowledge about the contents of~$\calS$. (The Niedermaier et al.~\cite{xxx} attacks, for example, exploit knowledge about the distribution of German last names.) In particular, our notions take as input an explicit distribution $\distr{U}{n}$ over~$[U]^n$.

%\jknote{We also want to capture the fact that finding the first false positive should not make it any easier to find subsequent false positives. This is not really captured by the Naor paper, either (at least not directly, though it may end up being implied by their definition).}
%
%\tsnote{See my comment in the intro.}

In Figure~\ref{fig:fp-filter} we give three distinct notions, characterized by what access the adversary has to the underlying hash functions used by the filter.  In the top notion, the hash functions are secret.  This is the setting addressed by NY, and it captures applications in which the hash functions are (say) secretly keyed PRFs, and in which no explicit, external interface to these is surfaced.
%
The bottom notion considers the case that the full code of the hash functions is known to the attacker.  This is the case, for example, when attacking the Bloom filters used in the Squid proxy~\cite{xxx}.
%
The middle notion is a compromise: it makes the hash functions
public, but assumes that the adversary only accesses them in a
black-box manner.  That is, the adversary does not exploit
structural features of the hash functions in its attack.  This
notion makes it easy to track the query complexity of an attack, and
also allows us to capture constructions in the random-oracle model.
\tsnote{This is the setting for the Neidermayer et al.\  attacks.
It's also the setting for ROM attacks.} \jnote{I don't think the
middle notion is interesting. I disagree regarding the attacks of
Neidermayer et al.: there, the hash functions are secret (though
they are based on public ``random oracles'' $g$ and $h$). The ROM
can be handled separately by giving all algorithms access to
the~RO.}\tsnote{You're correct; I misread some of the text.  It is
only the setup for the attack that needs access to the hash functions,
and this is just modeling what is likely to appear in the real
``encrypted'' BFs.}

For each experiment, we define an associated advantage
notion.  Namely, given an $(n,k,m)$-filter~$B=(\Hash,\Rep,\Qry)$,
distribution~$\distr{U}{n}$ and adversary~$A$, we define:
\begin{align*}
\AdvFPSecHash{B,t}{\distr{U}{n},A} &= \Prob{\ExpFPSecHash{B,t}{\distr{U}{n},A}=1} \\
\AdvFPPubHashBB{B,t}{\distr{U}{n},A} &= \Prob{\ExpFPPubHashBB{B,t}{\distr{U}{n},A}=1} \\
\AdvFPSecHash{B,t}{\distr{U}{n},A} &= \Prob{\ExpFPPubHash{B,t}{\distr{U}{n},A}=1}
\end{align*}
The probabilities are over the coins of the indicated experiment,
and the coins of the adversary.   In all experiments, we track the
time-complexity~$t$ of~$A$, as well as the number of set-membership
($\QryOracle$ oracle) queries~$q_{\mathrm{sm}}$ that it makes.   In
the second, we also track the number of hash-queries
$q_{\mathrm{h}}$ that the adversary makes.

We note that the distribution~$\distr{U}{n}$ must be independent of
the hash functions $h_1,h_2,\ldots,h_k$ that are used to create~$M$.
Otherwise, when (say) $\Sigma=\bits$, the distribution may only
assign positive probability to sets~$\calS$ that cause an abnormally
large number of bits in~$M$ to be set to 1.  Our security notions
enforce this independence.

\jnote{How do we want to define a ``secure'' Bloom filter (even informally)? Do we want to say that the probability of finding a false positive is negligible? Or that if we set the FP rate to $\epsilon$ then the best an attacker can do with $q$ hash queries is $1-(1-\epsilon)^q$ plus negligible?}
%
\tsnote{Formally, I don't think we can (and still be rooted in
reality).  Even informally it's hard, since the fact is that the
designer decides what is ``secure'' for her setting.  I think we
just say something like ``A bloom filter is
$(t,q_h,q_m,\epsilon)$-secure if no attacker running in time at
most~$t$, asking at most~$q_h$ hash queries and $q_m$ set-membership
queries, has probability more than~$\epsilon$ of finding a FP.''
Perhaps informally we say that a bloom filter is secure if, for
``reasonable'' $t,q_h,q_m$, the value of~$\epsilon$ is not
``significantly larger'' than the designer's target.  I guess that's
not far off from one of your suggestions.}

%\jnote{One thing that is nice about the NY notion is that you get a ``clean'' definition of security: no matter how many FPs the attacker has found so far, the probability that it can output a fresh FP is still $\epsilon$.}


\begin{figure}
\centering
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpFPSecHash{B,t}{\distr{U}{n},A}$}\\
$\calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \getsr \Rep^{\HashOracle}(\calS)$\\
$\calX \getsr A^{\QryOracle}(\calS)$\\
if $|\calX| < t$ or $\calX \cap \calS \neq \emptyset$ then \\
\nudge Ret 0\\
If $\exists x \in \calX$ s.t. $\Qry^{\HashOracle}(M,\tau,x)=0$ then\\
\nudge Ret 0\\
Ret 1
}
%
{
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
}
}
%%%%%%%
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpFPPubHashBB{B,t}{\distr{U}{n}, A}$}\\
$\calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \getsr \Rep^{\HashOracle}(\calS)$\\
$\calX \getsr A^{\QryOracle,\HashOracle}(\calS)$\\
if $|\calX| < t$ or $\calX \cap \calS \neq \emptyset$ then \\
\nudge Ret 0\\
If $\exists x \in \calX$ s.t. $\Qry^{\HashOracle}(M,\tau,x)=0$ then\\
\nudge Ret 0\\
Ret 1
}
%
{
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
}
}
%%%%%%%
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpFPPubHash{B,t}{\distr{U}{n}, A}$}\\
$\calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \gets \Rep^{\HashOracle}(\calS)$\\
$\calX \getsr A^{\QryOracle}(S,\{h_1,h_2,\ldots,h_k\})$\\
if $|\calX| < t$ or $\calX \cap \calS \neq \emptyset$ then \\
\nudge Ret 0\\
If $\exists x \in \calX$ s.t. $\Qry^{\HashOracle}(M,\tau,x)=0$ then\\
\nudge Ret 0\\
Ret 1
}
%
{
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
} } \caption{False-positive security (soundness) for an
$(n,k,m)$-filter~$B$ with private hash functions (top), black-box
queriable hash functions (middle), and public hash functions
(bottom). \jnote{Could also consider not giving $\calS$ to the
adversary.} } \label{fig:fp-filter}
\end{figure}


%%%%
% \if{0}
% \begin{figure}
% \centering
% \fpage{.6}{
% \hpagess{.6}{.35}
% {
% \experimentv{$\ExpFPSecHash{S,B}{A}$}\\
% $ \{h_1,h_2,\ldots,h_k\} \getsr \Hash$\\
% $M \gets \Rep^{\HashOracle}(S)$\\
% $x \getsr A^{\QryOracle}(S)$\\
% if $x \not\in S \wedge \Qry^{\HashOracle}(M,x)=1$ then\\
% \nudge Ret 1\\
% Ret 0
% }
% %
% {
% \oracle{$\QryOracle(x)$}\\
% Ret $\Qry^{\HashOracle}(M,x)$\\

% \medskip
% \oracle{$\HashOracle(i,x)$}\\
% Ret $h_i(x)$\\
% }
% }
% %%%%%%%
% \fpage{.6}{
% \hpagess{.6}{.35}
% {
% \experimentv{$\ExpFPPubHashBB{S,B}{A}$}\\
% $ \{h_1,h_2,\ldots,h_k\} \getsr \Hash$\\
% $M \gets \Rep^{\HashOracle}(S)$\\
% $x \getsr A^{\QryOracle,\HashOracle}(S)$\\
% if $x \not\in S \wedge \Qry^{\HashOracle}(M,x)=1$ then\\
% \nudge Ret 1\\
% Ret 0
% }
% %
% {
% \oracle{$\QryOracle(x)$}\\
% Ret $\Qry^{\HashOracle}(M,x)$\\

% \medskip
% \oracle{$\HashOracle(i,x)$}\\
% Ret $h_i(x)$\\
% }
% }
% %%%%%%%
% \fpage{.6}{
% \hpagess{.6}{.35}
% {
% \experimentv{$\ExpFPPubHash{S,B}{A}$}\\
% $ \{h_1,h_2,\ldots,h_k\} \getsr \Hash$\\
% $M \gets \Rep^{\HashOracle}(S)$\\
% $x \getsr A^{\QryOracle}(S,\{h_1,h_2,\ldots,h_k\})$\\
% if $x \not\in S \wedge \Qry^{\HashOracle}(M,x)=1$ then\\
% \nudge Ret 1\\
% Ret 0
% }
% %
% {
% \oracle{$\QryOracle(x)$}\\
% Ret $\Qry^{\HashOracle}(M,x)$\\

% \medskip
% \oracle{$\HashOracle(i,x)$}\\
% Ret $h_i(x)$\\
% }
% }
% \caption{False-positive security (soundness) for an $(n,k,m)$-filter~$B$ with private hash functions (top), black-box queriable hash functions (middle), and public hash functions (bottom). }
% \label{fig:fp-filter}
% \end{figure}
%\fi
%%%%%



\heading{Privacy of hash-based filters.} We now define privacy notions for $(n,k,m)$-filters.  Informally, the adversary is given the representation~$M$ of a secret set~$\calS$, and its goal is to guess one of the elements of~$\calS$.  Of course, if the distribution~$\distr{U}{n}$ has low min-entropy with respect to any particular element $x \in U$, then it will be relatively easy to guess~$x$.  To capture this min-entropy, let $p_{\mathrm{max}} = \max_{x \in U} \Prob{\calS \getsr \distr{U}{n}:\, x \in \calS}$ and define $\mu = -\log_2 p_{\mathrm{max}}$ to be the (element-wise) min-entropy of distribution $\distr{U}{n}$.

\tsnote{Explain need for $\MemOracle$, as a way to disambiguate true positives from false positives.}

We note that~$\calS$ must be independent of the hash functions~$h_1,h_2,\ldots,h_k$ used to create the representation of~$\calS$.  \tsnote{I thought this would be needed, but now I'm not sure it is. }

A weaker notion of privacy would not give~$M$ to the adversary.  However, this would not capture attacks such as those mounted by Neidermayer et al.\cite{XXX}, and would not speak to settings in which (say) an honest-but-curious third party is given the representation~$M$ in order to carry out some task on behalf of the holder of~$\calS$.

\begin{figure}
\centering
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpPrivSecHash{B}{\distr{U}{n}, A}$}\\
$\calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \getsr \Rep^{\HashOracle}(\calS)$\\
$x \getsr A^{\MemOracle,\QryOracle}(M)$\\
if $x\in \calS$ then Ret 1\\
Ret 0
}
%
{
\oracle{$\MemOracle(x)$}\\
Ret $[x \in \calS]$\\

\medskip
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
}
}
%%%%%%%%%%
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpPrivPubHashBB{B}{\distr{U}{n}, A}$}\\
$ \calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \getsr \Rep^{\HashOracle}(\calS)$\\
$x \getsr A^{\MemOracle,\QryOracle,\HashOracle}(M)$\\
if $x\in \calS$ then Ret 1\\
Ret 0\\
}
%
{
\oracle{$\MemOracle(x)$}\\
Ret $[x \in \calS]$\\

\medskip
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
}
}
%%%%%%%
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpPrivPubHash{B}{\distr{U}{n}, A}$}\\
$ \calS \getsr \distr{U}{n}$\\
$h_1,\ldots,h_k \getsr \Hash$\\
$(M,\tau) \getsr \Rep^{\HashOracle}(\calS)$\\
$x \getsr A^{\MemOracle,\QryOracle}(M,\{h_1,h_2,\ldots,h_k\})$\\
if $x\in \calS$ then Ret 1\\
Ret 0\\
}
%
{
\oracle{$\MemOracle(x)$}\\
Ret $[x \in \calS]$\\

\medskip
\oracle{$\QryOracle(x)$}\\
Ret $\Qry^{\HashOracle}(M,\tau,x)$\\

\medskip
\oracle{$\HashOracle(x)$}\\
Ret $\left(h_1(x),\ldots,h_k(x)\right)$\\
}
}
\caption{Set-privacy for an $(n,k,m)$-filter~$B$ for private (top), black-box queriable (middle),  and public (bottom) hash functions. \textcolor{cyan}{Revisit when the FP notion settles out.}}
\label{fig:fp-filter}
\end{figure}


\fi