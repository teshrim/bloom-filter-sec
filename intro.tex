\section{Introduction}
\label{sec:intro}

Data structures are a fundamental tool used throughout computer
science. Typically, when the behavior of a data structure (e.g., its
running time or its error probability) is analyzed, the analysis is
performed under the assumption that any inputs and queries to the
data structure are chosen non-adversarially and, in particular, are
independent of any internal randomness used by the data structure.
But as recently highlighted by Naor and Yogev~\cite{C:NaoYog15},
there are many natural scenarios where the input and/or queries may
be chosen \emph{adversarially} and \emph{adaptively}, based on
partial information and prior observations about the data structure.

In addition to the above, standard applications of data structures
assume that the party querying the data structure is fully trusted
and hence privacy concerns do not arise. But in many real-world
scenarios, a data structure is built from sensitive data.  Here, it
is important to understand what information is leaked by public
portions of the data structure and, minimally, to know that a
reasonable number of queries will not divulge ``too much more
information'' that was is implied by the answers to those queries.
Existing definitions say nothing about privacy in this sense.

These concerns motivate a careful and formal treatment of data
structures in an adversarial setting, where both \emph{correctness}
and \emph{privacy} are considered. We initiate such a treatment in
this work.

\heading{Formalizing abstract data structures.} Loosely speaking, we
formalize a data structure as a pair of algorithms $(\Rep,\Qry)$,
the representation and query-evaluation algorithms, respectively.
The randomized representation algorithm takes in an abstract
object~$S$ and produces public representation data $\pubaux$ and
private representation data $\privaux$.  Intuitively, $\pubaux$ is
public information, whereas $\privaux$ is meant to be known only to
the party running $\Qry$.  The deterministic query-evaluation
algorithm uses $\pubaux$ and $\privaux$ in order to (approximately)
evaluate a supported query $\qry$, which can in principle be an
arbitrary function.
%We say ``approximately'' evaluate permitted queries
%because $\pubaux$ and $\privaux$ may not contain all of the
%information sufficient to determine the true value of $\qry(S)$ for
%all~$S$.

As an example, consider the classic Bloom
filter~\cite{bloom1970space}. Here, $\pubaux$ consists of a bit
array~$M$ that represents a given set~$S$, as well as descriptions
of the hash functions $H_1,H_2,\ldots,H_k$ used to produce~$M$;
$\privaux$ is empty.  The supported queries are predicates of the
form $\qry_x(S)=1$ iff $x \in S$. The~$\Qry$ algorithm, on input
$M,H_1,\ldots,H_k$ and $\qry_x$, carries out the standard
Bloom-filter test to determine if $x \in S$. It is well known that
Bloom filters admit false-positive errors; thus, $\Qry$ only
approximates the true set-membership test.  Our correctness notion
will quantitatively capture the error, even in the presence of
attackers that adaptively try to induce one or more errors
(false-positives, in this case).

By shifting $H_1,H_2,\ldots,H_k$ from $\pubaux$ to $\privaux$, we do
not change the functionality, but we may very well change the
security profile of the resulting Bloom-filter variant.  Indeed,
this is the case.  When $H_1,H_2,\ldots,H_K$ are assumed to be
pseudorandom functions, we can prove that this variant achieves a
sort of ``semantic security'' privacy guarantee: the bit array~$M$
leaks a good approximation of~$|S|$, but nothing more.  (Such a
result is not possible for the standard Bloom filter, or any other
data structure with empty~$\privaux$.)

\heading{Case studies.} The example above offers a hint at the broad
range of abstract data structures our syntax supports.  In this
paper, we put the syntax and our security notions to work in several
case studies.  First, we analyze classical Bloom filters when the
hash functions are modeled as public random
oracles~\cite{broder2004network}, when the hash functions are
secretly-keyed PRFs~\cite{niedermeyer2014cryptanalysis,C:NaoYog15},
and when the hash functions are secretly keyed but fall short of
being PRFs, i.e., so-called ``linear-PRF'' hash
functions~\cite{kirsch2006less,niedermeyer2014cryptanalysis}.  Each
of these represents an in-use or proposed Bloom-filter variant. We
also consider garbled Bloom filters~\cite{CCS:DonCheWen13}, proposed
as a key piece of a private set-intersection protocol.

The linear-PRF Bloom filter illustrates how brittle constructions
can be depending on how they are used. When used to create a
representation of a \emph{set}, we prove that the linear-PRF Bloom
filter offers both correctness and privacy. However, when applied to
a \emph{multiset}, privacy is lost.  In fact, Niedermeyer et
al.~\cite{niedermeyer2014cryptanalysis} show that the linear-PRF
Bloom filter, when applied to a collection of bigrams derived from
German last names, admits a practical attack that reveals the names,
despite the fact that the hash functions are constructed from
secretly-keyed PRFs in a way that has been suggested in recent
literature on Bloom-filter variants~\cite{kirsch2006less}.

%We see our work as laying the foundation for a number of follow-on
%studies.  For example to study dynamic data structures, such as
%counting Bloom filters\cite{fan2000summary}, scalable Bloom
%filters~\cite{almeida2007scalable}, count-min
%sketches~\cite{cormode2005improved}, hierarchical Bloom
%filters~\cite{zhu2004hierarchical}, to name just a few in the
%extended Bloom filter family.  Another direction would be to explore
%the security of primitives built \emph{from} these sorts of data
%structures, such as content-distribution networks (where many
%servers propagate representations of their local cache to their
%neighbors), or cryptographic strike lists (a la early versions of
%the QUIC protocol) for nonce-replay protection.  The Bloom filter
%family alone has a wide range of practical applications, for example
%in large database query processing, routing algorithms for
%peer-to-peer networks, protocols for establishing linkages between
%medical-record databases, fair routing of TCP packets, and Bitcoin
%wallet
%synchronization~\cite{schnell2011novel,niedermeyer2014cryptanalysis,gervais2014privacy,nojima2009cryptographically,feng2001stochastic,reynolds2003efficient,byers2002informed,broder2004network}.




\ignore{We provide various security notions for these primitives...
FP with private, public representation coins; set privacy with
private, public representation coins; FP/set privacy in the
multi-representation setting (outsider attack, think CDNs); FP/set
privacy in the multi-representation setting with corruptions
(insider attack, think P2P networks) \tsnote{Those are just the ones
that come to mind now.  Could also consider authenticity of
representations, think CDN or P2P settings.}}

\heading{Related work.}
\input{related-work}
%\jnote{See the discussion of related work in
%Naor-Yogev. We should cite [LN93]. I can't tell how relevant [MNS11]
%is. [HW13] seems relevant, though I don't think it is captured by
%our current definitions.}

%\jnote{Discuss use of BFs and BF variants in a security-oriented
%setting. Also some prior work formalizing BFs in an adversarial
%environment. See commented out portion of the .tex}

\ignore{\heading{Related work: attacks}
\begin{itemize}
\item Niedermayer et al., ``Cryptanalysis of Basic Bloom Filters Used for Privacy-Preserving Record Linkage'', breaking privacy of secret-hash-function Bloom filters. \tsnote{Journal of Privacy and Confidentiality, 2014}
\item Gerbet, Kumar and Lauradoux, ``The power of evil choices in bloom filters''.
\tsnote{DSN'15: Looks like a real goldmine of related work!}
\item Crosby and Wallach, ``Denial of Service via Algorithmic Complexity Attacks'' \tsnote{Gives attacks on Squid}
\item Gao et al., ``Internet Cache Pollution Attacks and Countermeasures''
\end{itemize}}

\ignore{\heading{Related work: definitions(?)}
\begin{itemize}
\item Nojima and Kadobayashi, ``Cryptographically Secure Bloom Filters''.
\tsnote{Gives some security definitions for privacy. Quick scan, not super
clear what they achieve. The definition of client-privacy
 (Definition 1) for example, makes no sense to me.  Actually,
 likewise for server-privacy (Definition 2).  Both seem vague
 and thoroughly underspecified.}
\item Naor and Yogev
\item Eujin Goh, ``Secure Indexes'' \tsnote{A secure index can
    be used for set membership.  Builds a secret-key data
    structure (an Index) that allows searching for keyword~$w$
    if one holds the trapdoor $T_w$ for~$w$, where the trapdoor
    depends on the secret key.  Main construction uses
    traditional Bloom filters and a PRF.  Construction appears
    quite inefficient, needing a very long secret key, turning a
    keyword~$w$ into a bunch of PRF outputs, and then storing
    each of these PRF outputs in the BF.  Haven't read the full
    analysis; don't know if this was ever published. }
    \jnote{Never published. I think this work uses Bloom filters
    for encrypted search; I don't remember the paper having much
    to say about Bloom filters themselves.}
\end{itemize}

\heading{Related work: constructions}
\begin{itemize}
\item Bellovin and Cheswick, ``Privacy-Enhanced Searches Using
    Encrypted Bloom Filters''.
\item Kerschbaum , ``Public-Key Encrypted Bloom Filters with Applications to Supply Chain Integrity''.
\item S\"{a}rell\"{a} et al., ``BloomCasting: Security in Bloom Filter Based Multicast''.
\item Dong, Chen, Wen, ``When Private Set Intersection Meets Big Data: An Efficient and Scaleable Protocol'' \tsnote{``garbled bloom filters'', which actually store the set element by storing~$k$ xor-shares, one at each of the~$k$ hash indices (with care for reusing shares if hash collisions occur); also and``oblivious bloom intersection''}\tsnote{If the filter and the hash functions are public, there is a naive attack that works for some interesting parameters.  Pick~$k$ positions $i_1,i_2,\ldots,i_k$, xor strings at these positions to get a candidate~$x$.  If $h_{1}(x)=i_1 \wedge h_2(x)=i_2 \wedge \cdots \wedge h_k(x) = i_k$ then either $x \in S$ or~$x$ is a false-positive. (You don't have to do all~$k$ hashes, you can stop as soon as one fails to match the index.) Say you have a 1000-position filter ($m=1000$) and a 100-element set ($n=100$), and you are willing to tolerate a 1\% FP rate.  Then you'd set $k=4$ (a la Squid).  There are roughly $2^{35}$ subsets of 4 indices to consider, which is reasonable on a laptop these days.  (Also, since $n=100$ the expected number of trials is roughly $2^{28}$.) With the FP rate of 1\%, if you find a winner then almost certainly it is in the set.  Granted, this attack is naive, but notice that you \emph{cannot} mount it against a standard Bloom filter. Also notice that this attack might be significantly more efficient than brute force guessing over the universe of inputs (imagine that $U=\bits^{64}$), even if you know that the distribution deviates significantly from uniform. }
\item Tarkoma, Rothenberg, Lagerspetz ``Theory and Practice of Bloom Filters in Distributed Systems'' \tsnote{Great high-level coverage.  Only found preprint version though.}
\item Durham, Kantarcioglu, Xue, Kuzu, Malin ``Composite Bloom Filters for Secure Record Linkage'' \tsnote{Per-field BFs, sampled and composed into single BF that is then permuted by a secret random permtation.  No clear statement of the problem that is being solved.  Should pull full version and get details.}
\end{itemize}

\heading{Related work: tangential}
\begin{itemize}
\item Chang and Mitzenmacher ``Privacy Preserving Keyword Searches on Remote Encrypted Data''.
\item Mitzenmacher and Vadhan. ``Why Simple Hash Functions Work: Exploiting the Entropy in a Data Stream''.
\item Dodis et al. ``Fuzzy Extractors: How to Generate Strong Keys from Biometrics and Other Noisy Data'' \tsnote{Introduces ``secure sketches'', which is a representation of a single-element set that is information theoretically private (up to some function of the min-entropy of the element); only tangentially related to ``sketches'' as defined in the Bloom filter literature.}
\end{itemize}
}
