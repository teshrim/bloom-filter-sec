\section{Introduction}
\paragraph{Basic issues in Naor-Yogev. }
\begin{itemize}
\item The NY paper formalizes and investigates a security notion of limited interest
\item The proposed construction won't scale in common network applications (e.g. large CDN, P2P networks)
\item The main (im)possibility results are of theoretical interest only
\item The basic syntax and operation of their Bloom filter formalization is underdefined.  (This really irritates me.)  It isn’t clear that their Bloom filter formalization actually captures traditional Bloom filters! \tsnote{UPDATE (5/20/16): It is clear, now, that it \emph{does not}.} \tsnote{Also, why isn't the size of the representation somehow a part of the syntax or correctness/soundness conditions?  The fact that the representation is supposed to be small doesn't appear in the syntax anywhere so, syntactically, $B_1(S)=S$ and $B_2(S,x)=1 \Leftrightarrow x \in S$ works just fine, and is trivially secure by their definition! }
\item The ``adversarial-resilient Bloom Filter'' definition does not give the adversary access to the hash functions that created the representation; effectively, they are secret. (No matter if the [NY] syntax does or does not allow them to be encoded in the representation.)  This is odd and does not match reality, e.g. the popular Squid proxy uses plain MD5, splits the output into four chunks, and defines these as four hash functions.  Hardly secret. \tsnote{I did find a setting where Bloom filters use secret keys: privacy-preserving record linkage.  Here one wants to know if two databases contain information about the same person (abstractly, a set of record identifiers); you create a Bloom filter for each record, one for each unique identifier in some cases.  Suggested hash functions are $H_j(x) = g(x) + j\cdot h(x) \bmod m$ for $j=\{0,1,\ldots,k-1\}$, and $g,h$ are (e.g.) HMAC-SHA1 with different keys.  In practice, apparently the~$x$ are bigrams extracted from string identifiers (e.g. ``Thomas''); papers show how to break this ``encrypted'' Bloom filter by recovering a large fraction of names, via (roughly) frequency analysis.}
\item The ``unsteady representation'' does not seem well founded.  Not sure what Bloom filter-like data structures would have randomized querying procedures.  They mention the possibility of differential privacy mechanisms being incorporated into the querying procedure.  But what's the purpose of this, if the security definition gives the adversary the set~$S$?  (What are you keeping private?) 
\end{itemize}

\paragraph{Discussion. }
The traditional definition of Bloom filter soundness is information theoretic.  Effectively, it states that an IT adversary cannot find a false positive with probability more than some small value.  (That value is a function of the size of the set being represented, the representation size, the size of the universe, and the number of hash functions.)  This paper ([NY]) replaces the traditional IT notion with a computational notion, and casts it as a security experiment/goal.  Namely: given the set~$S$ that is represented by the Bloom filter, and access to an oracle for Bloom filter set-membership queries, an attacker must find a false positive.  

\jknote{Of course, [NY] discusses an info-theoretic notion also. So in some sense what is really new here is consideration of adaptivity...}
\tsnote{Yes, defining soundness with respect to explicit adversaries, who are also adaptive, does seem to be the novel piece.} 

Security relative to this notion appears to have some practical relevance.  [NY] points to whitelisting spam filters as an example.  That is, a Bloom filter is populated with set~$S$ of whitelisted email addresses, so a false positive translates to an attacker finding an email address that sneaks through the filter.  [NY] suggest a second example of content-caching servers that keep a Bloom filter of their own storage contents.  A false positive causes the server to search its local store, which is slow, only to find nothing. \tsnote{Question: in their security notions, the adversary wins by returning an $x^*$ that is not in~$S$ and is not one of the queries it asked to $B_2$; why the latter?  I understand why you shouldn't get credit for output something in~$S$ when~$S$ is given to you, but what if one of the queries to~$B_2$ is not in~$S$ and is a FP?  Shouldn't the adversary already win?}

To the first example, I suspect that spam filters are far more sophisticated than simply looking for whitelisted email addresses.  So the example feels artificial and overblown as practical motivation.  But more to the point, the security notion proposed by [NY] gives~$S$ ---the set of whitelisted email addresses--- to the adversary.  In practice, knowledge of these email addresses may be more damaging than creating a false-positive email address! (Keep in mind that the attacker wins, in this scenario, by creating any email address that passes the filter, even ones that bear no resemblance to real email addresses.)  From this perspective, the [NY] security definition is narrow in its scope, even assuming it says something useful about spam filtering.

\jknote{Yes, although another way to interpret this is that the adversary cannot find a false positive EVEN IF it had the whitelisted email addresses, even if in practice it does not.}
\tsnote{Fair enough. And this potentially unrealistic attack is preventable by a simple countermeasure.  That said, I'm not convinced the countermeasure is easy to deploy at scale, or that it meshes well with real deployments.  (See below.)  } \tsnote{Also, while the adversary is given~$S$ (the whitelisted email addresses), the hash functions used to create the Bloom filter are \emph{secret}.  I'm not convinced this is realistic.}

I am less critical of the second example, although this is mostly because I feel ignorant of how real CDNs operate.  However, I believe that the following is common.  Say I'm a caching server.  If my Bloom filter tells me that some requested content is not locally stored, then I check the Bloom filters of my network neighbors, these having been shared with me.  When a neighbor's Bloom filter is positive for the requested content, I contact that neighbor, and that neighbor only.  (If no neighbor has it, then I have to retrieve it from some slower and more distant source, like the Web.)  Now, the [NY] construction uses a PRP, meaning that a secret key is involved in evaluating the Bloom filter.  Thus, I need to know the secret keys of my neighbors in order to operate in the way just described.  This seems troublesome in practice, at CDN-sized scale. (Also in P2P networks, where peers are joining/leaving frequently.)  Thus, their solution to a (potentially) real problem seems to be a non-solution.

Note that there is at least one way to avoid this knowledge of secret keys issues.  Namely, assume that content-caching servers distinguish between Bloom filter queries that originate from outside the CDN (e.g. users’ browsers) and those from within (e.g. other servers), and that the attacker only makes the former type of queries.  Then each server could store its neighbors’ Bloom filters in the clear, as normal, and only use the PRP construction when responding to external queries.  I don’t know how realistic this separation of queries is, in practice, nor how easy it would be to deploy such a strategy.  I also don’t know if this restricted attack scenario is interesting or plausible.

Moving on, the paper spends too much time asking and answering questions that are interesting to theoreticians, only.  Specifically, the relationship between the existence of one-way functions and Bloom filters that are secure relative to their definition.

\jknote{Agreed. =)}

Finally, the Bloom filter formalization that is given does not obviously capture traditional Bloom filters!  Traditionally, both Bloom filter population and querying procedures use hash functions $h_1,h_2,...h_k$ that each map from some universe $U$ to $[m]$, where $m$ is the bit-length of the array $M$ that represents a given set $S$.  These hash functions are missing from the authors’ definition, apparently swept into the algorithms $B_1$ (population) and $B_2$ (querying).  I don't really understand how, though. They allow $B_1$ to be randomized, which would let $B_1$ pick the hash functions.  But the security definition provides no way for $B_1$ to share these with $B_2$.  So how would $B_2$ be able to process set-membership queries?  (Note: based on how they define the size of the representation $M$, $B_1$ cannot include descriptions of $h_1, h_2, \ldots , h_k$ in $M$.  They really mean $M$ to be the array that represents the set $S$.)  In general, their formalization of the Bloom filter primitive is badly underdefined.

\jknote{You had me for a minute...but actually I think the representations of the hash functions ARE included in the representation. This is not explicit, but is consistent with the way they state their positive result in the computational setting (“...if there is a Bloom filter using $m$ bits of memory, then there is a strongly resilient Bloom filter using $m + \lambda$ bits of memory…”); more importantly, it seems explicit from their treatment of the IT case.}

\tsnote{Perhaps.  I'm still not convinced.  If you read the text preceding the statement of Theorem 1.2 (page 4), you'll see their discussion of lower bounds on “memory” required for a given set size and FP rate.  This bound is the classical one for Bloom filters and speaks only of the (bit)size of the array.  It says nothing about the description of the hash functions needed to populate the array.  In fact, this lower bound is for any method of representing set $S$ in a bit array, not only for hashing-based methods.
%
Maybe they are defining “memory” differently in this discussion than in their theorems.  This would be annoying, and I think it isn't the case.  So for the moment, I'll continue to hold my position on this.
%
(I'll also point out that they explicitly say that $B_1$ outputs “compressed representation of [set $S$] $B_1(S)=M$”.  But it would be easy to have small sets $S$ for which the bitsize of the array is smaller than the total bitsize of $S$, yet the combined bitsize of array + hash functions is larger.)
%
But for the sake of argument, say that their representation size~$m$ does include the description of the hash functions.  To me, this is a clunky way to handle this matter.  For one thing, it doesn't play well with analysis in the ROM, which is the traditional way to establish results relating FP rate, set size and array size for Bloom filters.  (I know there are standard model assumptions that suffice, too.)  For another, this way of accounting makes unnecessarily hard to answer the two important operational questions: how big is the array, and how many hash function calls do I need to make per item/query?  
% 
I'll agree that this syntactic issue does not matter for theorem statements such as ``...if there is a Bloom filter using $m$ bits of memory, then there is a strongly resilient Bloom filter using $m + \lambda$ bits of memory…''.  In fact, this statement would be true for the traditional way of defining Bloom filter memory (array size), since the extra space is basically for a PRP key.}

\tsnote{UPDATE (5/20/16): I'm now convinced that what they call memory size~$m$, when applied to real Bloom filters, is \emph{only} the size of the bit array, and does \emph{not} include a description of the hash functions.  The memory size comparisons that are made throughout the paper are to classical memory lowerbounds that account only for the bitsize of the array. Concretely: in the abstract they say ``... we show there exists a Bloom filter for sets of size~$n$ and error~$\epsilon$, that is secure against~$t$ queries and uses only $O(n\log\frac{1}{\varepsilon} + t)$ bits of memory.  In comparison, $n \log\frac{1}{\varepsilon}$ is the best possible under a non-adaptive adversary.''  The latter bound is the classical one.   If you look in the paper for this actual result, it's in Theorem 6.2.   So, we look at the theorem and the construction that proves it.  It uses a specially constructed function~$g$ and a Cuckoo hashing dictionary~$D$, consisting of two tables~$T_1,T_2$ and two hash functions~$h_1,h_2$.  Here is a quote from the proof ``The final memory of the Bloom filter is the memory of~$D$ and the representation of~$g$.  The dictionary~$D$ consists of $O(n)$ cells, each of size $|g(x)|=O(\log\frac{1}{\varepsilon})$ bits and therefore~$D$ and~$g$ together can be represented using $O(n\log\frac{1}{\varepsilon} + t)$ bits'', where earlier they say ``the representation of~$g$ requires $O(t)$ bits''. (Note that this~$g$ is only used to determined \emph{what} is stored at the indicated array/table positions, not \emph{where}.)  Thus they are not considering the size of the hash functions~$h_1,h_2$ that are used to indicated which cells of~$D$ need to be altered.   So, as I said, I'm now convinced that their Bloom filter syntax does not actually capture real Bloom filters.}

Finally, the security definitions do not give the adversary access to the hash functions that created the representation; effectively, they are secret. \tsnote{Unless they are assuming that the hash functions are fixed, part of the algorithms $B_1$ and $B_2$?  In which case, $A$~knows them too; and the security notion gives~$S$ to~$A$, so...}  This strikes me as quite odd, and seems to miss the point that finding a false positive should be hard, in practice, even if you know that the hash functions are, say,  $h_i(X)=\mathrm{SHA256}(\langle i \rangle \concat X)$.  (If the hash functions are not kept secret, the whole [NY] security notion kind of unravels; the adversary already is given~$S$, so if it knows the hash functions then you might as well just give it the whole Bloom filter data structure.) \tsnote{In the Squid proxy, the hash functions are not secret: you can download the source code, and the docs~\footnote{http://wiki.squid-cache.org/SquidFaq/CacheDigests} (available online even!) tell you that ``The protocol design allows for a variable number of hash functions (k). However, Squid employs a very efficient method using a fixed number - four. {\bf Rather than computing a number of independent hash functions over a URL Squid uses a 128-bit MD5 hash of the key (actually a combination of the URL and the HTTP retrieval method) and then splits this into four equal chunks. Each chunk, modulo the digest size (m), is used as the value for one of the hash functions - i.e. an index into the bit array.}  Note: As Squid retrieves objects and stores them in its cache on disk, it adds them to the in-RAM index using a lookup key which is an MD5 hash - the very one discussed above. This means that the values for the Cache Digest hash functions are already available and consequently the operations are extremely efficient!''  (``Key'' here just means hash input, as in ``key-value pair''.)}

\paragraph{Obvious Tasks. }

\begin{enumerate}
\item Get a solid handle on how Bloom filters are used in practice, and what generalizations have gained traction in the academic literature. (See wikipedia, a section of which I've pasted in at the end of this doc.) \tsnote{5/23: lots of uses to consider --CDNs/load balancers, peer-to-peer networks, medical record linkage, model checking, etc.  Might be good to try to characterize by, say, setting (distributed vs. local), typical BF instance size (e.g. local whitelist of emails = ``small'', CDN server-cache summary = ``large''), security concerns, and estimated severity of security violation (e.g. FP at CDN server = ``low'' or ``medium'', deanonymizing medical record = ``high'').  Just throwing out ideas...}
\item Work out a proper “cryptographic” syntax for the Bloom filter primitive.  (Possibly something more general, see number 5.)
\item Establish security notions that are appropriate for broader use cases (e.g. privacy of~$S$, privacy of the representation~$M$, authenticity of representation~$M$), single Bloom filter vs.\ multiple Bloom filter settings (e.g. attacking a CDN by querying multiple servers)
\item Develop constructions that are secure and are supported by a practice-informed argument for deployment
\item Break some stuff :-)
\item Expand from Bloom filters to “Approximate Set Membership” primitives, i.e. more general compressed representations (of sets) that admit queries  Note: this should be done only if it makes sense -- i.e. it doesn't explode the scope, it doesn't send us off too far into theoryland.
\item Expand more generally to hash-based approximate set operations, e.g.\ minhash methods for approximating $J(A,B)$ for sets $A,B$.  Note: this should be done only if it makes sense -- i.e. it doesn't explode the scope, it doesn't send us off too far into theoryland.
\end{enumerate}

\jknote{I think privacy is an interesting question. I also thought that both privacy and soundness could be considered both in the setting when the representation is “hidden” from the adversary (as in the paper under discussion) as well as in the setting where the representation is public. [Ahh...I see you have some similar thoughts further below.] Some thoughts about this:
(1) Privacy in the “hidden” setting. Here the idea would be that queries about whether $x \in S$ reveal only the result of the query and nothing else;  
(2) Privacy in the “public” setting. Here we could assume the elements of~$S$ have high entropy, and~$S$ has low density, and require that the attacker cannot find~$x \in S$ even when given the representation(!) I see no reason why this should not be achievable in the random oracle model, or possibly even in the standard model;
(3) Soundness in the “public” setting. Note that if the soundness is 0.5 then the attacker can simply guess random elements until it finds a false positive. So one way to approach this is to look at negligible soundness error only. Or, in the random oracle model, you could count queries to the RO and bound the attacker's success as a function of the number of queries it makes.
}

\paragraph{From the Wikipedia page...}
Google BigTable, Apache HBase and Apache Cassandra use Bloom filters to reduce the disk lookups for non-existent rows or columns. Avoiding costly disk lookups considerably increases the performance of a database query operation.[7][8]
The Google Chrome web browser used to use a Bloom filter to identify malicious URLs. Any URL was first checked against a local Bloom filter, and only if the Bloom filter returned a positive result was a full check of the URL performed (and the user warned, if that too returned a positive result).[9][10]
The Squid Web Proxy Cache uses Bloom filters for cache digests.[11]
Bitcoin uses Bloom filters to speed up wallet synchronization.[12][13]
The Venti archival storage system uses Bloom filters to detect previously stored data.[14]
The SPIN model checker uses Bloom filters to track the reachable state space for large verification problems.[15]
The Cascading analytics framework uses Bloom filters to speed up asymmetric joins, where one of the joined data sets is significantly larger than the other (often called Bloom join[16] in the database literature).[17]
The Exim mail transfer agent (MTA) uses bloom filters in its rate-limit feature.[18]

