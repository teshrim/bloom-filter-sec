\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.
In addition to correctness, we establish notions of privacy for
set-representations.

\tsnote{flesh out}

\subsection{Correctness Against Adaptive Adversaries}

First, a set~$S$ is chosen according to some
distribution~$\distr{}{}$ over the collection of multisets
$\mathcal{S}$ associated to~$\Pi$, and then
$\Rep(S)$ is run to generate $(M,\pubaux,\privaux)$.
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\calS$ and $\pubaux$~as input; it is
provided an oracle~$\TestOracle$ that, on input a query~$q \in \calQ$,
computes (and returns) the result of $\Qry(M,\privaux,q)$, and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.

The notion is lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).



\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,\distr{}{},r}{A}$}\\
$S \getsr \distr{}{}$\\
$\mathrm{err}\gets 0$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(S,\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
if $a \neq q(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$(a,\mathrm{err})$
}
}
\caption{Correctness of~$\Pi$ against (adaptive) adversary~$A$, when
  the represented multiset~$S$ is sampled according to distribution~$\distr{}{}$.}
\label{fig:correctness}
\end{figure}

In this experiment we track the time-complexity (relative to some
implied model of computation) and query-complexity (i.e., number of
queries) of the adversary~$A$.
We define the advantage of adversary~$A$ in the correctness experiment as
$\AdvCorrect{\Pi,\distr{}{},r}{A} = \Prob{\ExpCorrect{\Pi,\distr{}{}, r,}{A}=1}$.
Overloading notation, we write $\AdvCorrect{\Pi,\distr{}{},r}{t,q}$ for the maximum over all~$t$-time adversaries that ask at most~$q$ queries. Roughly, we say that a
set-representation~$\Pi$ is $(t,q,r,\epsilon')$-correct if $\AdvCorrect{\Pi,\distr{}{},r}{t,q} \leq \epsilon'$.
\jnote{Note that test includes err because we don't want to penalize
the adversary's resources for computing $q(S)$.}

\heading{Comparison to Naor-Yogev definition.}
The style of the Naor-Yogev (NY) definition is slightly different. \jnote{Even if we ignore
all the other issues, chief among them the inconsistent way it treats the hash functions.}
Lifting their definition to our setting, the experiment is similar but the attacker
succeeds only if it outputs a single query $q$ for which $\Qry( M,
\aux, q) \neq q(S)$  \emph{subject to the restriction that it did not query $q$ to its $\Qry$ oracle}.
It will be interesting to understand formally the relation between our definition and theirs.
Here are some observations:
\begin{enumerate}
\item Say a scheme is $\epsilon_{NY}$-secure under the NY definition.
Then we claim it is $(t,q, r, q\epsilon_{NY}/r)$-correct. To see
this, assume not. Then there is a $t$-time attacker~$A$ that, with
probability at least $\epsilon'=q\epsilon_{NY}/r$,
issues~$q$ Test-queries of which at least~$r$ cause errors. If we simply run~$A$ and then choose a random Test-query to
output, we succeed with probability at least $\epsilon' \cdot r/q$. It is unclear whether this is tight.

\item There is a contrived scheme that is $(t, q, 2, 0)$-correct but not NY-secure at all; the basic idea is
to have a set-representation that always has \emph{exactly one} false positive that is easy to find
given the public information.

\item If a scheme is $(t,q,1,\epsilon)$-correct, then it is also $\epsilon$-secure under the NY definition,
at least for attackers making at most $q-1$ queries.
This is immediate.
\end{enumerate}
\jnote{Case of $r=1$ does not necessarily imply anything for $r>1$: can have a scheme
where finding the first false positive is hard but then the rest are easy; can have
a scheme where finding one false positive is easy but there do not exist
any more false positives.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Notions of Privacy}
One can also consider the orthogonal notion of privacy for a
data structure. \jnote{I am still hoping that we can treat data structures
at an even more general level than set-representation data structures.}
We formalize two types of privacy notions. The first captures
a ``one-wayness'' property that (informally) requires that
the representation of some set~$S$ does not make it any easier for an attacker
to learn an element of~$S$.
The second definition
captures a flavor of semantic security, essentially requiring that $M$ and $\pub$ do
not leak any information about the set~$S$ they represent beyond
what is captured in some explicit leakage function.

\jnote{OK, from here on I specialize to set-representation data structures\ldots}

\jnote{I don't know if I like the one-wayness definition any more; it seems a bit
ad hoc. And why do we care about one-wayness, anyway? Instead, I give
two versions of semantic security.}

%The one-wayness definition (see
%Figure~\ref{fig:privacy-ow}) is parameterized by a distribution $\distr{}{}$ over
%sets. A set $S$ is chosen from this distribution, and a representation
%$(\pubaux,\privaux)$ of that set is computed.
%In one experiment ($\ExpPriv{\Pi,\distr{}{}}$), an attacker~$A$ is given $\pubaux$;
%in a second experiment ($\ExpPrivNull{\Pi,\distr{}{}}{}$), it is not.
%For a given $\distr{}{}$ and $A$, we define the advantage measure
%$\AdvPriv{\Pi,\distr{}{}}{A} \bydef
%\left| \Prob{\ExpPriv{\Pi,\distr{}{}}{A}=1} - \Prob{\ExpPrivNull{\Pi,\distr{}{}}{P}=1} \right|$.
%We track the
%time-complexity~$t$ and number of oracle queries~$q$ made by the adversary.
%\jnote{Change. Compare $A$ in the 1-world to $P$ in the 0-world. Security means
%there exists a $P$
%such that for all~$A$ the following holds: $A$ in the 1-world does not do much better than
%$P$ in the 0-world. Note that there is a canonical $P$ that does best, information-theoretically.}
%\jnote{I now wonder whether we should allow test queries in both worlds.}
%
%\begin{figure}[hbtp]
%\centering
%\fpage{.6}{
%\hpagess{.5}{.45}
%{
%\experimentv{$\ExpPriv{\setprim,\distr{}{}}{A}$}\\
%$S \getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$z \getsr A^{\TestOracle}(M,\pubaux)$\\
%if $z \in S$ then return 1\\
%return 0\\
%
%\medskip
%\experimentv{$\ExpPrivNull{\Pi, \distr{}{}}{P}$} \\
%$S \getsr \distr{}{}$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$z \getsr P()$\\
%if $z \in S$ then return 1\\
%return 0\\
%%\medskip
%%\experimentv{$\ExpWPriv{\setprim,\distr{}{}}{A}$}\\
%%$S \getsr \distr{}{}$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%%$z \getsr A^{\TestOracle}(\pubaux)$\\
%%if $z \in S$ then Return 1\\
%%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M,\pubaux,\privaux,q)$\\
%return~$a$
%}
%}
%\caption{Experiments for a ``one-wayness'' style privacy
%  definition.}
%\label{fig:privacy-ow}
%\end{figure}

%\jnote{Actually, I am unsure whether a one-wayness definition is independently
%interesting if we have (and can achieve) a semantic-security definition.}
%\tsnote{Depends on what information we decide is okay to leak.  A
%  classical BF gives an~$M$ that leaks a good approximation of $|S|$.
%Do we want that to be considered semantically secure?  On the other
%hand, a classical BF is one-way in the ROM, up to the min-entropy of
%the distribution over~$S$.  So that may already give an interesting
%example that fits in the gap.} \jnote{Yes. Semantic security
%is parameterized by the leakage function, and so might or might not imply
%one-wayness.}
Our semantic-secure definition is simulation-based (see
Figure~\ref{fig:privacy-ss}).
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs a set $S$ and is given the
public information $\pubaux$ that results from running $\Rep(S)$.\footnote{We do not give $A$ an oracle for
issuing queries; since $A$ knows the set~$S$, such an oracle would be superfluous.
\jnote{Interestingly,
if we provided a query oracle in the real world and an oracle that returns the correct
answer in the simulated world, then the definition seems to imply
correctness also.}}
\tsnote{Need to pass state between stages if you want~$A$ to know the
  set it previously sampled, right?} \jnote{Yes, unless by convention all algorithms
  are stateful. (smiley face). Whatever you prefer is fine with me.}
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $\pubaux$ output
by a simulator $\Sim$ that is only given ${\sf leak}(S)$ for some (possibly randomized)
function~${\sf leak}$. \jnote{Could also give $\Sim$ a membership oracle.}
For a given function ${\sf leak}$, adversary $A$, and simulator~$\Sim$,
we define the advantage measure
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim} \bydef
\left| \Prob{\ExpPrivSSreal{\Pi}{A}=1} -
\Prob{\ExpPrivSSsim{\Pi,{\sf leak}}{A, \Sim}=1} \right|$.
Informally, a scheme $\Pi$ is secure with respect to some leakage function ${\sf leak}$
if for all $A$ there is a simulator $\Sim$ such that
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim}$ is small.
Intuitively, this means that the public portion
$\pubaux$ of the representation of a set~$S$ leaks~${\sf leak}(S)$ and nothing more.
\jnote{Need to relate running time of $\Sim$ to running time of~$A$.}

\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\Pi}{A}$}\\
$S \getsr A()$ \\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(M,\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\Pi, {\sf leak}}{A, \Sim}$}\\
$S \getsr A()$\\
$(M,\pubaux) \getsr \Sim({\sf leak}(S))$ \\
return $A(M,\pubaux)$\\

%\medskip
%\experimentv{$\ExpPrivSSsim{\Pi}{A, \calS}$}\\
%$S \gets A()$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(M',\pubaux) \gets \calS(\pubaux)$ \\
%return $A(M',\pubaux)$\\
}
}
\caption{Experiments for a ``semantic-security'' style privacy
  definition.}
\label{fig:privacy-ss}
\end{figure}


%
%\begin{figure}[htbp]
%\centering
%\hfpages{.495}
%{
%\hpagess{.49}{.49}
%{
%
%\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr A^{\TestOracle}(M,\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%
%\medskip
%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr P(\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%{
%\hpagess{.49}{.49}
%{
%\experimentv{$\ExpPrivSS{\setprim,\distr{}{},P}{A}$}\\
%$S\getsr \distr{}{}$\\
%$b \getsr \bits$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(M_1,\pubaux,\privaux) \getsr \Rep(S)$\\
%$M_0 \getsr P(\pubaux)$\\
%%$(f,v) \getsr \getsr A^{\TestOracle}(M,\pubaux)$\\
%$b' \getsr A^{\TestOracle}(M_b,\pubaux)$\\
%%if $f(S)=v$ then\\
%if $b'=b$ then\\
%\nudge Return 1\\
%Return 0\\
%%
%%\medskip
%%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%%$S\getsr \distr{}{}$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%%$(f,v) \getsr P(\pubaux)$\\
%%if $f(S)=v$ then\\
%%\nudge Return 1\\
%%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M_1,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%\caption{Semantic-security style privacy
%  definitions. \textcolor{red}{On the left, need to protect against
%    adversary outputting
%    things like $f(S)=M$.  One idea is to forbid~$f$ such that
%    $f(S)=f(S')$ for all $S,S'$ in the support of $\distr{}{}$.}
%    }
%\label{fig:privacy-ss}
%\end{figure}

\subsection{Relationships Among Notions}\todo{Decide which of these
  statements should have formal theorems and proofs.}
Clearly we have priv $\Rightarrow$ wpriv, tightly and by an obvious
reduction.  The converse is not true, however: consider the previous
construction in which $M = S \setminus \{x\}$ for a random $x \in S$,
and $\pubaux=\privaux=\emptystring$.  We note that this construction
also serves to separate the priv notion from our notion of
correctness.  (It is $1-(1/|S|)$ correct, but has no priv-security.)

Curiously, the trivial scheme that sets $M=S$, $\pubaux=\privaux=\emptystring$ does seem to be both correct and wpriv-secure (up to the element-wise min-entropy of~$S$).
\tsnote{In general, reducing correctness to wPriv is syntactically possible, but not clear how to connect the success events.  In the one-wayness game, you win by outputing an element of~$S$; but how does this lead to finding an error-inducing query?  Maybe the one-wayness attacker makes no queries, and just outputs a point~$z$.  Then the correctness attacker does what?  Ask some query~$q_z$ related to~$z$ ---remember, queries are not syntactically restricted to set membership!--- and see if the oracle returns the true value $q_z(S)$?  This is pretty complicated... and it's not clear how to do a quality reduction.   }
\tsnote{Likewise the other way: the one-wayness adversary does not get~$S$, and yet needs to provide this to the correctness adversary.}


\todo{We should be able to show...}
\begin{itemize}
\item ss-rep $\Rightarrow$ priv \emph{for set-multiplicity data
    structures} \tsnote{but probably not in general}
\item \{w\}priv $\not\Rightarrow$ ss-rep
\end{itemize}
