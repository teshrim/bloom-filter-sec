
\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.
In addition to correctness, we establish notions of privacy for
set-multiplicity data structures.
All adversaries are stateful by default.

\tsnote{flesh out}

\subsection{Correctness Against Adaptive Adversaries}

First, the adversary $A$ outputs a set $S$ from among those supported by
the data structure. Then,
$\Rep(S)$ is run to generate $(\pubaux,\privaux)$.
%\jnote{I thought we were going to let
%the adversary choose~$S$?} \tsnote{Email discussion. At this point,
%I'm happy just making~$S$ part of the experiment ``name'', the
%way~$\distr{}{}$ is now.  The distribution seems not to matter for
%correctness, at least for the schemes we consider.} \jnote{If instead we choose $S$ from ${\cal D}$, then
%is it clear we want to give $S$ to~$A$?} \tsnote{For \emph{correctness},
%yes, if for no other reason than to allow~$A$ to know when it asking
%a pointless query.  But also it will make no difference to the
%provable correctness of the constructions we consider.}
%\jnote{OK, I think I would just change the definitions so that the attacker
%chooses $S$.}
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\pubaux$~as input; it is also
provided with access to an oracle~$\TestOracle$ that, on input a query~$\qry \in \calQ$,
returns $\Qry(\pubaux,\privaux,q)$ and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.
The notion can be lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).

%\jnote{I changed $\TestOracle$ so that it outputs $(a, q(S))$ rather
%than $(a,\mathrm{err})$. Note that the latter can be computed from the former,
%and in the general case I think the former is what we
%want.}\tsnote{This may have implications for the efficiency of
%reductions, I'm not sure. The reason I introduced the $\mathrm{err}$
%counter and returned this was to avoid~$A$ be forced to evaluate
%queries of unknown computational cost.} \jnote{Not sure what you mean. If the
%oracle returns $q(S)$ then $A$ doesn't have to evaluate it. All $A$ has to do
%is an equality check to see if $a=q(S)$ or not.}



\begin{figure}[htp]
\centering
\fpage{.65}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,r}{A}$}\\
$S \getsr A$\\
$\mathcal{C} \gets \emptyset$; $\mathrm{err}\gets 0$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$\perp \getsr A^{\TestOracle}(\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(\qry)$}\\
if $\qry \in \mathcal{C}$ then Return $\bot$\\
$a \gets \Qry(\pubaux,\privaux,\qry)$\\
if $a \neq \qry(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
$\mathcal{C} \gets \mathcal{C} \cup \{\qry\}$ \\
Return~$a$
}
}
\caption{Correctness of~$\Pi$ against an (adaptive) adversary~$A$.}
\label{fig:correctness}
\end{figure}

In this experiment we track the time complexity (relative to some
model of computation) and query complexity (i.e., number of
queries to $\TestOracle$) of~$A$.
We define the advantage of adversary~$A$ in the correctness experiment as
$\AdvCorrect{\Pi,r}{A} = \Prob{\ExpCorrect{\Pi,r}{A}=1}$.
\tsnote{drop} Overloading notation, we write $\AdvCorrect{\Pi,r}{t,q}$ for the maximum over
all~$t$-time adversaries that ask at most~$q$ queries. We say a
data structure~$\Pi$ is $(t,q,r,\epsilon)$-correct if $\AdvCorrect{\Pi,r}{t,q} \leq \epsilon$.

\todo{Explain that the case of $r=1$ does not necessarily imply anything for $r>1$: can have a scheme
where finding the first false positive is hard but then the rest are easy; can have
a scheme where finding one false positive is easy finding more is hard (e.g.,
because there do not exist
any more).}


\newcommand{\leak}{{\sf leak}}
\subsection{Privacy}

One can also consider the orthogonal notion of \emph{privacy} for a data structure.
We specialize to the case of set-membership data structures, and
focus on sets rather than multisets. Before continuing, it will be useful to define a notion that captures the difficulty
of guessing an element in a set chosen from some distribution.
Let ${\cal D}$ be a distribution over subsets of some universe ${\cal X}$. The \emph{point-wise
min-entropy} of ${\cal D}$ is defined as
\[H_\infty({\cal D}) \bydef -\log \max_{x \in {\cal X}} \Pr[S \getsr {\cal D} : x \in S].\]

\tsnote{I think we should switch the order of presentation, since we
  use the SS notion much more. }
We formalize two types of privacy notions. The first captures
a ``one-wayness'' property that (informally) requires that
the public representation of a set~$S$ does not make it any easier for an attacker
to learn elements of~$S$.  The second definition
captures a flavor of semantic security, essentially requiring that $\pub$ does
not leak any information about the set~$S$ it represents beyond
what is captured by some explicit leakage function.

Our one-wayness notion is defined in Figure~\ref{fig:privacy-ow}.
The relevant experiment begins by first sampling a set $S$ according to a distribution
${\cal D}$. Then $\Rep(S)$ is run to obtain $\pubaux,\privaux$, and the attacker~$A$ is given
$\pubaux$. The attacker succeeds if it is able to output an element of~$S$.
We define the advantage of adversary~$A$ in this experiment\footnote{We
remark that one could consider augmenting experiment $\ExpPrivOW{\Pi,{\cal D}}{A}$ by additionally
giving $A$ access to an oracle for testing membership in~$S$. However, it is not hard
to see that allowing $q$ such queries can increase $A$'s advantage by at most a factor of~$q$.} as
$\AdvPrivOW{\Pi,\mathcal{D}}{A} = \Prob{\ExpPrivOW{\Pi,{\cal D}}{A}=1}$ and,
overloading notation, we write $\AdvPrivOW{\Pi,{\cal D}}{t}$ for the maximum of this value over
all~$t$-time adversaries.
We remark that an attacker can trivially achieve advantage
$2^{-H_\infty({\cal D})}$ by outputting the element most likely to be in~$S$.

\begin{figure}[hbtp]
\centering
\fpage{.25}{
%\hpagess{.5}{.45}
%{
\experimentv{$\ExpPrivOW{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A(\pubaux)$\\
if $z \in S$ then return 1\\
return 0
%}
%{
%\oracle{$\TestOracle(q)$}\\
%Return $\Qry(\pubaux,\privaux,q)$
%}
}
\caption{Experiment for a ``one-way'' style privacy
  definition.}
\label{fig:privacy-ow}
\end{figure}


Our semantic-security definition is based on the simulation paradigm (see Figure~\ref{fig:privacy-ss}).
\jnote{Here it would not be hard to write the definition for general data structures, not
specifically set-multiplicity data structures.}
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs a set $S$  and is given the
public information $\pubaux$ that results from running $\Rep(S)$.
%\footnote{We do not give $A$ an oracle for issuing queries;
%since $A$ knows the set~$S$, such an oracle would be superfluous.
%\jnote{Interestingly,
%if we provided a query oracle in the real world and an oracle that returns the correct
%answer in the simulated world, then the definition seems to imply
%correctness also.}}
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $\pubaux$ output
by a simulator $\Sim$ that is only given ${\sf leak}(S)$ for some (possibly randomized)
function~${\sf leak}$.
For a given function ${\sf leak}$, adversary $A$, and simulator~$\Sim$,
we define the advantage
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim} \bydef
\left| \Prob{\ExpPrivSSreal{\Pi}{A}=1} -
\Prob{\ExpPrivSSsim{\Pi,{\sf leak}}{A, \Sim}=1} \right|$.
Informally, a scheme $\Pi$ is secure with respect to some leakage function ${\sf leak}$
if for all efficient $A$ there is a simulator $\Sim$ such that
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim}$ is small.
Intuitively, this means that the public portion
$\pubaux$ of the representation of a set~$S$ leaks~${\sf leak}(S)$ and nothing more.
\jnote{The definition
as stated does not really work well in the presence of (programmable) random oracles.}
\jnote{In the current definition there is (intentionally) no bound on the simulator's
running time.}

\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\Pi}{A}$}\\
$S \getsr A$ \\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\Pi, {\sf leak}}{A, \Sim}$}\\
$S \getsr A$\\
$\pubaux \getsr \Sim({\sf leak}(S))$ \\
return $A(\pubaux)$\\
}
}
\caption{Experiments for the ``semantic-security'' privacy
  definition.}
\label{fig:privacy-ss}

\end{figure}
\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\Pi,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$ \\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(S,\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\Pi, {\sf leak}, \distr{}{}}{A, \Sim}$}\\
$S \getsr \distr{}{}$\\
$\pubaux \getsr \Sim({\sf leak}(S))$ \\
return $A(S,\pubaux)$\\
}
}
\caption{Experiments for the ``distibutional semantic-security'' privacy
  definition.}
\label{fig:privacy-dss}
\end{figure}

\heading{Relationships between the notions.}
Intuition suggests that semantic security should imply one-wayness. And, indeed, this is
the case---provided one is careful with the technical details.
For a distribution ${\cal D}$
over subsets of some universe ${\cal X}$, define
\[\tilde H_\infty({\cal D} \mid {\sf leak}) \bydef -\log \sum_{r \in R} \Pr_{S \getsr {\cal D}}[{\sf leak}(S)=r] \cdot
\max_{x \in {\cal X}} \Pr_{S \getsr {\cal D}}[x \in S \mid {\sf leak}(S)=r].\]

\jnote{Make theorem statements consistent -- get rid of $(t, \epsilon)$-style.}

\begin{theorem}
Let $\Pi$ be a set-representation data structure that is $(t, \epsilon)$-semantically secure
with respect to leakage ${\sf leak}:2^{\cal X} \rightarrow R$.
Then $\AdvPrivOW{\Pi,\mathcal{D}}{t} \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})} + \epsilon$.
\end{theorem}
\begin{proof}
Fix a distribution ${\cal D}$ and an attacker
$A$ running in time $t$, and let $\delta \bydef \AdvPrivOW{\Pi,\mathcal{D}}{A}$.
Define an attacker $A'$ who behaves as follows: first, it samples a set $S$ according to
distribution~${\cal D}$. Then, given $\pubaux$, it runs $A(\pubaux)$ to obtain an element~$z$.
Finally, it outputs~1 if and only if $z \in S$.

It is immediate that $\Pr[\ExpPrivSSreal{\Pi}{A'}=1] = \delta$. Semantic
security of $\Pi$ implies that there exists a simulator $\Sim$ for which
$\ExpPrivSSsim{\Pi, {\sf leak}}{A', \Sim} \geq \delta - \epsilon$. However,
the only information
that $\Sim$
and $A'$ jointly have about the set~$S$ that was sampled is ${\sf leak}(S)$; thus,
we must have $\ExpPrivSSsim{\Pi, {\sf leak}}{A', \Sim} \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}$.
This implies
$\delta \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}+\epsilon$ as desired.
\end{proof}

%\todo{SS$\Rightarrow$OW?
%  (Probably not in general, but perhaps for set-multiplicity data
%  structures that satisfy certain assumptions on the qnuery set.)
%  SS$\not\Rightarrow$ correctness; the converse is already shown by
%  the example I gave earlier, i.e. $M=S\setminus\{x\}$ for random $x
%  \in S$.} \jnote{Since we can set $\pubaux$ to be the emptystring, I think
%  it is trivial to see that privacy does not imply correctness.}
