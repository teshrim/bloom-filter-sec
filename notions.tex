\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.   
In addition to correctness, we establish three notions of privacy for
set-representations. The first two have the flavor of a one-wayness game, in which the adversary is given the auxiliary information~$\pubaux$ and, either the representation~$M$ or the abillity to call an oracle that evaluates $\Qry(M,\pubaux,\privaux,q)$ on queries~$q$ of its choosing.  At the end of this game, the adversary must output some point in the set represented by~$M$.  In the second privacy notion, the adversary presents two sets~$S_0$ and $S_1$ (of equal size) and a representation~$M$ is created for one of these two. Then the adversary is given $\pubaux$ and oracle access to $\Qry(M,\pubaux,\privaux,q)$, and it must decide which set~$M$ represents.

\subsection{Correctness Against Adaptive Adversaries}

First, a set~$S$ is chosen according to some
distribution~$\distr{}{}$ over the collection of multisets
$\mathcal{S}$ associated to~$\Pi$, and then
$\Rep(S)$ is run to generate $(M,\pubaux,\privaux)$.
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\calS$ and $\pubaux$~as input; it is
provided an oracle~$\TestOracle$ that, on input a query~$q \in \calQ$,
computes (and returns) the result of $\Qry(M,\privaux,q)$, and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.

The notion is lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).



\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpCorrect{\setprim,\distr{}{},r}{A}$}\\
$S \getsr \distr{}{}$\\
$\mathrm{err}\gets 0$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(S,\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
if $a \neq q(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$(a,\mathrm{err})$
}
}
\caption{Correctness of~$\Pi$ against (adaptive) adversary~$A$, when
  the represented multiset~$S$ is sampled according to distribution~$\distr{}{}$.} 
\label{fig:correctness}
\end{figure}

In this experiment we track the time-complexity (relative to some
implied model of computation) and query-complexity (i.e., number of
queries) of the adversary~$A$.  
We define the advantage of adversary~$A$ in the correctness experiment as 
$\AdvCorrect{\Pi,\distr{}{},r}{A} = \Prob{\ExpCorrect{\Pi,\distr{}{}, r,}{A}=1}$.
Overloading notation, we write $\AdvCorrect{\Pi,\distr{}{},r}{t,q}$ for the maximum over all~$t$-time adversaries that ask at most~$q$ queries. Roughly, we say that a
set-representation~$\Pi$ is $(t,q,r,\epsilon')$-correct if $\AdvCorrect{\Pi,\distr{}{},r}{t,q} \leq \epsilon'$.  


\heading{Comparison to Naor-Yogev definition.}
The style of the Naor-Yogev (NY) definition is slightly different. \jnote{Even if we ignore
all the other issues, chief among them the inconsistent way it treats the hash functions.}
Lifting their definition to our setting, the experiment is similar but the attacker
succeeds only if it outputs a single query $q$ for which $\Qry( M,
\aux, q) \neq q(S)$  \emph{subject to the restriction that it did not query $q$ to its $\Qry$ oracle}.
It will be interesting to understand formally the relation between our definition and theirs.
Here are some observations:
\begin{enumerate}
\item Say a scheme is $\epsilon_{NY}$-secure under the NY definition.
Then we claim it is $(t,q, r, q\epsilon_{NY}/r)$-correct. To see
this, assume not. Then there is a $t$-time attacker~$A$ that, with
probability at least $\epsilon'=q\epsilon_{NY}/r$,
issues~$q$ Test-queries of which at least~$r$ cause errors. If we simply run~$A$ and then choose a random Test-query to
output, we succeed with probability at least $\epsilon' \cdot r/q$. It is unclear whether this is tight.

\item There is a contrived scheme that is $(t, q, 2, 0)$-correct but not NY-secure at all; the basic idea is
to have a set-representation that always has \emph{exactly one} false positive that is easy to find
given the public information.

\item If a scheme is $(t,q,1,\epsilon)$-correct, then it is also $\epsilon$-secure under the NY definition,
at least for attackers making at most $q-1$ queries.
This is immediate.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Notions of Privacy}
One can also consider the orthogonal notion of privacy for a
set-representation.  We formalize two types of privacy notions. One
type captures a kind of one-wayness of the representation~$M$,
relative to the allowed queries.  Informally, that given the
representation and the public information, it is hard to guess any
value in the represented set~$S$.  The other type captures a kind of
semantic security, essentially requiring that $(M,\pub)$ and th
allowed queries do not leak any (non-trivial) information about the set~$S$.
%\jnote{Actually, I am now unsure what the right way to define it is. It seems natural
%to give the attacker $\aux$. But if we give them $\aux$ and $M$ then they have everything.
%If $S$ has high min-entropy (element-wise), then it might still be
%possible to have security here.}\tsnote{I think we will always bump up
%against the (element-wise) min-entropy of the distribution over~$S$,
%at least in the ``one-wayness'' style definition that I had been imagining. }

In Figure~\ref{fig:privacy-ow} we give two ``one-wayness''
definitions, in which the adversary must output an element of the
set~$S$ represented by~$M$.  The first version (priv) gives the
adversary~$(M,\pubaux)$ as input; the second (wpriv) gives
only~$\pubaux$.  The wpriv-notion captures leakage about the elements
of~$S$ that results from observing the results of allowed queries.
The stronger priv-notion captures what is jointly leaked, by the
representation~$M$ and the allowed queries, about set elements.  We
define advantage measures $\AdvPriv{\Pi,\distr{}{}}{A} =
\Prob{\ExpPriv{\Pi,\distr{}{}}{A}=1}$ and $\AdvWPriv{\Pi,\distr{}{}}{A} =
\Prob{\ExpWPriv{\Pi,\distr{}{}}{A}=1}$; in both we track the
time-complexity~$t$ and number of oracle queries~$q$ made by the adversary.

\tsnote{Wet paint...}
In Figure~\ref{fig:privacy-ss} we give a definition capturing semantic-security (of sets)
with respect to the full representation (ss-rep).  In the ``real''
setting (ss-rep-1), a set~$S$ is sampled according to distribution~$\distr{}{}$,
and the adversary is given the~$M,\pubaux$ that result from running
$\Rep(S)$.  It is also provided oracle access to $\TestOracle$.
The adversary must output a function~$f\colon\calS \to \bits^*$ and a
string~$v$ such that $f(S)=v$.  In the ``fake'' setting (ss-rep-0) a
predictor~$P$ is given $\pubaux$ only, and no $\TestOracle$ access.  It
also outputs an $f,v$ pair such that $f(S)=v$.  The advantage of~$A$ (over~$P$)
is $\AdvPrivSS{\Pi,\distr{}{}}{A,P} =
\Prob{\ExpPrivSSreal{\Pi,\distr{}{}}{A}=1} -
\Prob{\ExpPrivSSsim{\Pi,\distr{}{}}{P}=1}$.  To prevent trivial wins
for~$A$, e.g. $f(S)=M$, we forbid~$A$ from outputting~$f$
such that $f(S)=f(S')$ for all $S,S'$ in the support of the
distribution $\distr{}{}$.


\begin{figure}[htbp]
\centering
\fpage{.495}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPriv{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(M,\pubaux)$\\
if $z \in S$ then Return 1\\
Return 0\\

\medskip
\experimentv{$\ExpWPriv{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(\pubaux)$\\
if $z \in S$ then Return 1\\
Return 0\\
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
Return~$a$
}
}
\caption{``One-wayness'' style privacy
  definitions. The bottom is a weaker notion, because the representation
  is hidden from the adversary.} 
\label{fig:privacy-ow}
\end{figure}

%
\begin{figure}[htbp]
\centering
\hfpages{.495}
{
\hpagess{.49}{.49}
{

\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
$S\getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$(f,v) \getsr A^{\TestOracle}(M,\pubaux)$\\
if $f(S)=v$ then\\
\nudge Return 1\\
Return 0\\

\medskip
\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
$S\getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$(f,v) \getsr P(\pubaux)$\\
if $f(S)=v$ then\\
\nudge Return 1\\
Return 0\\
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
Return~$a$\\
}
}
{
\hpagess{.49}{.49}
{
\experimentv{$\ExpPrivSS{\setprim,\distr{}{},P}{A}$}\\
$S\getsr \distr{}{}$\\
$b \getsr \bits$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$(M_1,\pubaux,\privaux) \getsr \Rep(S)$\\
$M_0 \getsr P(\pubaux)$\\
%$(f,v) \getsr \getsr A^{\TestOracle}(M,\pubaux)$\\
$b' \getsr A^{\TestOracle}(M_b,\pubaux)$\\
%if $f(S)=v$ then\\
if $b'=b$ then\\
\nudge Return 1\\
Return 0\\
%
%\medskip
%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr P(\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M_1,\pubaux,\privaux,q)$\\
Return~$a$\\
}
}
\caption{Semantic-security style privacy
  definitions. \textcolor{red}{On the left, need to protect against
    adversary outputting
    things like $f(S)=M$.  One idea is to forbid~$f$ such that
    $f(S)=f(S')$ for all $S,S'$ in the support of $\distr{}{}$.}
    } 
\label{fig:privacy-ss}
\end{figure}

\subsection{Relationships Among Notions}\todo{Decide which of these
  statements should have formal theorems and proofs.}
Clearly we have priv $\Rightarrow$ wpriv, tightly and by an obvious
reduction.  The converse is not true, however: consider the previous
construction in which $M = S \setminus \{x\}$ for a random $x \in S$,
and $\pubaux=\privaux=\emptystring$.  We note that this construction
also serves to separate the priv notion from our notion of
correctness.  (It is $1-(1/|S|)$ correct, but has no priv-security.)  

Curiously, the trivial scheme that sets $M=S$, $\pubaux=\privaux=\emptystring$ does seem to be both correct and wpriv-secure (up to the element-wise min-entropy of~$S$).
\tsnote{In general, reducing correctness to wPriv is syntactically possible, but not clear how to connect the success events.  In the one-wayness game, you win by outputing an element of~$S$; but how does this lead to finding an error-inducing query?  Maybe the one-wayness attacker makes no queries, and just outputs a point~$z$.  Then the correctness attacker does what?  Ask some query~$q_z$ related to~$z$ ---remember, queries are not syntactically restricted to set membership!--- and see if the oracle returns the true value $q_z(S)$?  This is pretty complicated... and it's not clear how to do a quality reduction.   }
\tsnote{Likewise the other way: the one-wayness adversary does not get~$S$, and yet needs to provide this to the correctness adversary.}


We should be able to show
\begin{itemize}
\item ss-rep $\Rightarrow$ priv \emph{for set-multiplicity data
    structures} \tsnote{but probably not in general}
\item \{w\}priv $\not\Rightarrow$ ss-rep
\end{itemize}



