\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.
In addition to correctness, we establish three notions of privacy for
set-representations. The first two have the flavor of a one-wayness
game, in which the adversary seeks to return an element of the
represented set~$S$.  The final notion is reminiscent of
semantic-security for encryption schemes, and attempts to capture the
intuition that the representation~$M$ and the public
information~$\pubaux$ should not leak any efficiently computable
information about~$S$.

\tsnote{flesh out}

\subsection{Correctness Against Adaptive Adversaries}

First, a set~$S$ is chosen according to some
distribution~$\distr{}{}$ over the collection of multisets
$\mathcal{S}$ associated to~$\Pi$, and then
$\Rep(S)$ is run to generate $(M,\pubaux,\privaux)$.
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\calS$ and $\pubaux$~as input; it is
provided an oracle~$\TestOracle$ that, on input a query~$q \in \calQ$,
computes (and returns) the result of $\Qry(M,\privaux,q)$, and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.

The notion is lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).



\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.6}{.35}
{
\experimentv{$\ExpCorrect{\setprim,\distr{}{},r}{A}$}\\
$S \getsr \distr{}{}$\\
$\mathrm{err}\gets 0$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(S,\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
if $a \neq q(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$(a,\mathrm{err})$
}
}
\caption{Correctness of~$\Pi$ against (adaptive) adversary~$A$, when
  the represented multiset~$S$ is sampled according to distribution~$\distr{}{}$.}
\label{fig:correctness}
\end{figure}

In this experiment we track the time-complexity (relative to some
implied model of computation) and query-complexity (i.e., number of
queries) of the adversary~$A$.
We define the advantage of adversary~$A$ in the correctness experiment as
$\AdvCorrect{\Pi,\distr{}{},r}{A} = \Prob{\ExpCorrect{\Pi,\distr{}{}, r,}{A}=1}$.
Overloading notation, we write $\AdvCorrect{\Pi,\distr{}{},r}{t,q}$ for the maximum over all~$t$-time adversaries that ask at most~$q$ queries. Roughly, we say that a
set-representation~$\Pi$ is $(t,q,r,\epsilon')$-correct if $\AdvCorrect{\Pi,\distr{}{},r}{t,q} \leq \epsilon'$.


\heading{Comparison to Naor-Yogev definition.}
The style of the Naor-Yogev (NY) definition is slightly different. \jnote{Even if we ignore
all the other issues, chief among them the inconsistent way it treats the hash functions.}
Lifting their definition to our setting, the experiment is similar but the attacker
succeeds only if it outputs a single query $q$ for which $\Qry( M,
\aux, q) \neq q(S)$  \emph{subject to the restriction that it did not query $q$ to its $\Qry$ oracle}.
It will be interesting to understand formally the relation between our definition and theirs.
Here are some observations:
\begin{enumerate}
\item Say a scheme is $\epsilon_{NY}$-secure under the NY definition.
Then we claim it is $(t,q, r, q\epsilon_{NY}/r)$-correct. To see
this, assume not. Then there is a $t$-time attacker~$A$ that, with
probability at least $\epsilon'=q\epsilon_{NY}/r$,
issues~$q$ Test-queries of which at least~$r$ cause errors. If we simply run~$A$ and then choose a random Test-query to
output, we succeed with probability at least $\epsilon' \cdot r/q$. It is unclear whether this is tight.

\item There is a contrived scheme that is $(t, q, 2, 0)$-correct but not NY-secure at all; the basic idea is
to have a set-representation that always has \emph{exactly one} false positive that is easy to find
given the public information.

\item If a scheme is $(t,q,1,\epsilon)$-correct, then it is also $\epsilon$-secure under the NY definition,
at least for attackers making at most $q-1$ queries.
This is immediate.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Notions of Privacy}
One can also consider the orthogonal notion of privacy for a
data structure. \jnote{I am still hoping that we can treat data structures
at an even more general level than set-representation data structures.}
We formalize two types of privacy notions. The first captures
a ``one-wayness'' property, which (informally) requires that an attacker
cannot learn any value in a set $S$ given its
representation $M$ along with the public information~$\pub$.  The second definition
captures a flavor of semantic security, essentially requiring that $M$ and $\pub$ do
not leak any (non-trivial) information about the set~$S$ they represent.
%\jnote{Actually, I am now unsure what the right way to define it is. It seems natural
%to give the attacker $\aux$. But if we give them $\aux$ and $M$ then they have everything.
%If $S$ has high min-entropy (element-wise), then it might still be
%possible to have security here.}\tsnote{I think we will always bump up
%against the (element-wise) min-entropy of the distribution over~$S$,
%at least in the ``one-wayness'' style definition that I had been imagining. }

\jnote{OK, from here on I specialize to set-representation data structures\ldots}
The one-wayness definition (see
Figure~\ref{fig:privacy-ow}) is parameterized by a distribution $\distr{}{}$ over
sets. \jnote{I got rid of the weaker definition, since it seems unnatural to me.}
A set $S$ is chosen from this distribution, and a representation
$(M,\pubaux,\privaux)$ of that set is computed.
In one experiment ($\ExpPriv{\Pi,\distr{}{}}$), the attacker is given $M, \pubaux$;
in a second experiment (${\bf Exp}^{\rm null}_{\Pi,\distr{}{}}$), it is not.
In both cases, the attacker is given
access to an oracle via which it can
issue queries and obtain the corresponding responses.
For a given $\distr{}{}$ and $A$, we define the advantage measure
$\AdvPriv{\Pi,\distr{}{}}{A} \bydef
\left| \Prob{\ExpPriv{\Pi,\distr{}{}}{A}=1} - \Prob{{\bf Exp}^{\rm null}_{\Pi,\distr{}{}}(A)=1} \right|$.
We track the
time-complexity~$t$ and number of oracle queries~$q$ made by the adversary.


\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPriv{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(M,\pubaux)$\\
if $z \in S$ then return 1\\
return 0\\

\medskip
\experimentv{${\bf Exp}^{\rm null}_{\Pi, \distr{}{}}(A)$} \\
$S \getsr \distr{}{}$\\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}()$\\
if $z \in S$ then return 1\\
return 0\\
%\medskip
%\experimentv{$\ExpWPriv{\setprim,\distr{}{}}{A}$}\\
%$S \getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$z \getsr A^{\TestOracle}(\pubaux)$\\
%if $z \in S$ then Return 1\\
%Return 0\\
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(M,\pubaux,\privaux,q)$\\
return~$a$
}
}
\caption{Experiments for a ``one-wayness'' style privacy
  definition.}
\label{fig:privacy-ow}
\end{figure}

\jnote{I'm not sure whether we want to limit consideration to ``high-entropy''
distributions
and require the probability of finding an element to be low,
or whether we want to allow arbitrary distributions but compare the attacker's
success probability when given $(M, \pub)$ to its success probability when given nothing.
The latter seems cleaner, so I took that approach here.}

\jnote{Actually, I am unsure whether a one-wayness definition is independently
interesting if we have (and can achieve) a semantic-security definition.}

Our semantic-secure definition is simulation-based (see
Figure~\ref{fig:privacy-ss}).
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs a set $S$ and is given the
$M,\pubaux$ that result from running $\Rep(S)$.
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $M, \pubaux$ output
by a simulator $\calS$ that is only given ${\sf leak}(S)$. \jnote{Note
there is no need for a query oracle here, since the attacker knows the set. Interestingly,
if we provide a query oracle in the real world and an oracle that returns the correct
answer in the simulated world, then this definition seems to imply correctness also.}
For a given function ${\sf leak}$, adversary $A$, and simulator~$\calS$,
we define the advantage measure
$\AdvPrivSS{\Pi,{\sf leak}}{A,\calS} \bydef
\left| \Prob{\ExpPrivSSreal{\Pi}{A}=1} -
\Prob{\ExpPrivSSsim{\Pi,{\sf leak}}{A, \calS}=1} \right|$.
Informally, a scheme $\Pi$ is secure with respect to some leakage ${\sf leak}$
if there is a simulator $\calS$ such that $\AdvPrivSS{\Pi,{\sf leak}}{A,\calS}$ is small
for all~$A$. Intuitively, this means that the public portion
$M, \pubaux$ of the representation of a set~$S$ leaks only ${\sf leak}(S)$. 

\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\Pi}{A}$}\\
$S \gets A()$ \\
$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(M,\pubaux)$\\

\medskip
\experimentv{$\ExpPrivSSsim{\Pi, {\sf leak}}{A, \calS}$}\\
$S \gets A()$\\
$(M,\pubaux) \getsr \calS({\sf leak}(S))$ \\
return $A(M,\pubaux)$\\
%\medskip
%\experimentv{$\ExpWPriv{\setprim,\distr{}{}}{A}$}\\
%$S \getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$z \getsr A^{\TestOracle}(\pubaux)$\\
%if $z \in S$ then Return 1\\
%Return 0\\
}
%
{}
}
\caption{Experiments for a ``semantic-security'' style privacy
  definition.}
\label{fig:privacy-ss}
\end{figure}


%
%\begin{figure}[htbp]
%\centering
%\hfpages{.495}
%{
%\hpagess{.49}{.49}
%{
%
%\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr A^{\TestOracle}(M,\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%
%\medskip
%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr P(\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%{
%\hpagess{.49}{.49}
%{
%\experimentv{$\ExpPrivSS{\setprim,\distr{}{},P}{A}$}\\
%$S\getsr \distr{}{}$\\
%$b \getsr \bits$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(M_1,\pubaux,\privaux) \getsr \Rep(S)$\\
%$M_0 \getsr P(\pubaux)$\\
%%$(f,v) \getsr \getsr A^{\TestOracle}(M,\pubaux)$\\
%$b' \getsr A^{\TestOracle}(M_b,\pubaux)$\\
%%if $f(S)=v$ then\\
%if $b'=b$ then\\
%\nudge Return 1\\
%Return 0\\
%%
%%\medskip
%%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%%$S\getsr \distr{}{}$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%%$(f,v) \getsr P(\pubaux)$\\
%%if $f(S)=v$ then\\
%%\nudge Return 1\\
%%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M_1,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%\caption{Semantic-security style privacy
%  definitions. \textcolor{red}{On the left, need to protect against
%    adversary outputting
%    things like $f(S)=M$.  One idea is to forbid~$f$ such that
%    $f(S)=f(S')$ for all $S,S'$ in the support of $\distr{}{}$.}
%    }
%\label{fig:privacy-ss}
%\end{figure}

\subsection{Relationships Among Notions}\todo{Decide which of these
  statements should have formal theorems and proofs.}
Clearly we have priv $\Rightarrow$ wpriv, tightly and by an obvious
reduction.  The converse is not true, however: consider the previous
construction in which $M = S \setminus \{x\}$ for a random $x \in S$,
and $\pubaux=\privaux=\emptystring$.  We note that this construction
also serves to separate the priv notion from our notion of
correctness.  (It is $1-(1/|S|)$ correct, but has no priv-security.)

Curiously, the trivial scheme that sets $M=S$, $\pubaux=\privaux=\emptystring$ does seem to be both correct and wpriv-secure (up to the element-wise min-entropy of~$S$).
\tsnote{In general, reducing correctness to wPriv is syntactically possible, but not clear how to connect the success events.  In the one-wayness game, you win by outputing an element of~$S$; but how does this lead to finding an error-inducing query?  Maybe the one-wayness attacker makes no queries, and just outputs a point~$z$.  Then the correctness attacker does what?  Ask some query~$q_z$ related to~$z$ ---remember, queries are not syntactically restricted to set membership!--- and see if the oracle returns the true value $q_z(S)$?  This is pretty complicated... and it's not clear how to do a quality reduction.   }
\tsnote{Likewise the other way: the one-wayness adversary does not get~$S$, and yet needs to provide this to the correctness adversary.}


\todo{We should be able to show...}
\begin{itemize}
\item ss-rep $\Rightarrow$ priv \emph{for set-multiplicity data
    structures} \tsnote{but probably not in general}
\item \{w\}priv $\not\Rightarrow$ ss-rep
\end{itemize}
