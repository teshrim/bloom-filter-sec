\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.
In addition to correctness, we establish notions of privacy for
set-multiplicity data structures.

\tsnote{flesh out}

\subsection{Correctness Against Adaptive Adversaries}

First, a set~$S$ is chosen according to some
distribution~$\distr{}{}$ over the collection of multisets
$\mathcal{S}$ associated with~$\Pi$, and then
$\Rep(S)$ is run to generate $(\pubaux,\privaux)$. \jnote{I thought we were going to let
the adversary choose~$S$?}
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\pubaux$~as input; it is also
provided with access to an oracle~$\TestOracle$ that, on input a query~$q \in \calQ$,
returns $\Qry(\pubaux,\privaux,q)$ and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.
\jnote{If we are specializing the set-multiplicity data structures then we
may as well have the attacker query $x$ which gets translated to query $q_x$. If
we are trying to be more general then we should be consistent.} \jnote{Why does $A$ output
anything at all?}

The notion is lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).



\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,\distr{}{},r}{A}$}\\
$S \getsr \distr{}{}$\\
$\mathrm{err}\gets 0$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(S,\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(\pubaux,\privaux,q)$\\
if $a \neq q(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$(a,\mathrm{err})$
}
}
\caption{Correctness of~$\Pi$ against an (adaptive) adversary~$A$, when
  the represented multiset~$S$ is sampled according to distribution~$\distr{}{}$.}
\label{fig:correctness}
\end{figure}

In this experiment we track the time complexity (relative to some
model of computation) and query complexity (i.e., number of
queries to $\TestOracle$) of~$A$.
We define the advantage of adversary~$A$ in the correctness experiment as
$\AdvCorrect{\Pi,\distr{}{},r}{A} = \Prob{\ExpCorrect{\Pi,\distr{}{}, r,}{A}=1}$.
Overloading notation, we write $\AdvCorrect{\Pi,\distr{}{},r}{t,q}$ for the maximum over
all~$t$-time adversaries that ask at most~$q$ queries. We say that a
set-representation data structure~$\Pi$ is $(t,q,r,\epsilon')$-correct if $\AdvCorrect{\Pi,\distr{}{},r}{t,q} \leq \epsilon'$.
\jnote{Note~$q$ is overloaded here.}
%\jnote{Note that test includes err because we don't want to penalize
%the adversary's resources for computing $q(S)$.}

\heading{Comparison to Naor-Yogev definition.}
The style of the Naor-Yogev definition is different from ours.
Lifting their definition\footnote{They focus specifically on set-representation data structures
with no false negatives, and consider worst-case choice of~$S$. Their data-structure syntax
also does not distinguish public and private portions of the representation. Nevertheless, it is straightforward
to extend their definition to more closely match ours.}
to our setting,
the experiment is similar but the attacker
succeeds only if it outputs a (single) query $q$ for which
$\Qry(\pubaux,\privaux, q) \neq q(S)$  \emph{subject to the restriction that it did not
previously query $q$ to its $\TestOracle$ oracle} (see Figure~\ref{fig:NYcorrectness}).

\begin{figure}[htp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,\distr{}{},r}{A}$}\\
$S \getsr \distr{}{}$\\
$\mathcal{Q} \gets \emptyset$\\
%$\mathrm{err}\gets 0$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$q \getsr A^{\TestOracle}(S,\pubaux)$\\
if $\left(q \not \in \mathcal{Q}\right.$ and \\
\nudge $\left.\Qry(\pubaux,\privaux,q) \neq q(S)\right)$ \\
\nudge \nudge then Return 1\\
Return 0
}
%
{
\oracle{$\TestOracle(q)$}\\
$a \gets \Qry(\pubaux,\privaux,q)$\\
$\mathcal{Q} \gets \mathcal{Q} \cup \{q\}$ \\
%\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
Return~$(a,q(S))$
}
}
\caption{The Naor-Yogev definition of
correctness, adapted to our setting.}
\label{fig:NYcorrectness}
\end{figure}

It will be interesting to understand formally the relation between our
definition and theirs.
Here are some observations:
\begin{enumerate}
\item Say a scheme is $\epsilon_{NY}$-secure under the NY definition.
Then we claim it is $(t,q, r, q\epsilon_{NY}/r)$-correct. To see
this, assume not. Then there is a $t$-time attacker~$A$ that, with
probability at least $\epsilon'=q\epsilon_{NY}/r$,
issues~$q$ Test-queries of which at least~$r$ cause errors. If we simply run~$A$ and then choose a random Test-query to
output, we succeed with probability at least $\epsilon' \cdot r/q$. It is unclear whether this is tight.

\item There is a contrived scheme that is $(t, q, 2, 0)$-correct but not NY-secure at all; the basic idea is
to have a set-representation that always has \emph{exactly one} false positive that is easy to find
given the public information.

\item If a scheme is $(t,q,1,\epsilon)$-correct, then it is also $\epsilon$-secure under the NY definition,
at least for attackers making at most $q-1$ queries.
This is immediate.
\end{enumerate}
\jnote{Case of $r=1$ does not necessarily imply anything for $r>1$: can have a scheme
where finding the first false positive is hard but then the rest are easy; can have
a scheme where finding one false positive is easy but there do not exist
any more false positives.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Privacy}
\jnote{Define appropriate notion of min-entropy for our setting.
Define one-wayness.}

One can also consider the orthogonal notion of \emph{privacy} for a
data structure. \jnote{I am still hoping that we can treat data structures
at an even more general level than set-representation data structures.}
We formalize two types of privacy notions. The first captures
a ``one-wayness'' property that (informally) requires that
the representation of some set~$S$ does not make it any easier for an attacker
to learn an element of~$S$.
The second definition
captures a flavor of semantic security, essentially requiring that $M$ and $\pub$ do
not leak any information about the set~$S$ they represent beyond
what is captured in some explicit leakage function.

\jnote{OK, from here on I specialize to set-representation data structures\ldots}


Our semantic-secure definition is simulation-based (see Figure~\ref{fig:privacy-ss}).
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs a set $S$  and is given the
public information $\pubaux$ that results from running $\Rep(S)$.
\footnote{We do not give $A$ an oracle for issuing queries;
since $A$ knows the set~$S$, such an oracle would be superfluous.
\jnote{Interestingly,
if we provided a query oracle in the real world and an oracle that returns the correct
answer in the simulated world, then the definition seems to imply
correctness also.}}
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $\pubaux$ output
by a simulator $\Sim$ that is only given ${\sf leak}(S)$ for some (possibly randomized)
function~${\sf leak}$.
For a given function ${\sf leak}$, adversary $A$, and simulator~$\Sim$,
we define the advantage measure
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim} \bydef
\left| \Prob{\ExpPrivSSreal{\Pi}{A}=1} -
\Prob{\ExpPrivSSsim{\Pi,{\sf leak}}{A, \Sim}=1} \right|$.
Informally, a scheme $\Pi$ is secure with respect to some leakage function ${\sf leak}$
if for all $A$ there is a simulator $\Sim$ such that
$\AdvPrivSS{\Pi,{\sf leak}}{A,\Sim}$ is small.
Intuitively, this means that the public portion
$\pubaux$ of the representation of a set~$S$ leaks~${\sf leak}(S)$ and nothing more.
\jnote{Need to relate running time of $\Sim$ to running time of~$A$.}
\todo{Introduce OW notion, if we keep it}
\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\Pi}{A}$}\\
$(S,\st) \getsr A()$ \\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(\st,\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\Pi, {\sf leak}}{A, \Sim}$}\\
$(S,\st) \getsr A()$\\
$(\st,\pubaux) \getsr \Sim({\sf leak}(S))$ \\
return $A(\st,\pubaux)$\\
}
}
\caption{Experiments for a ``semantic-security'' style privacy
  definition.}
\label{fig:privacy-ss}
\end{figure}

\begin{figure}[hbtp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivOW{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A^{\TestOracle}(\pubaux)$\\
if $z \in S$ then return 1\\
return 0\\
}
{
\oracle{$\TestOracle(q)$}\\
eturn $\Qry(\pubaux,\privaux,q)$
}
}
\caption{Experiments for a ``one-way'' style privacy
  definition.}
\label{fig:privacy-ow}
\end{figure}

%
%\begin{figure}[htbp]
%\centering
%\hfpages{.495}
%{
%\hpagess{.49}{.49}
%{
%
%\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr A^{\TestOracle}(M,\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%
%\medskip
%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%$S\getsr \distr{}{}$\\
%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(f,v) \getsr P(\pubaux)$\\
%if $f(S)=v$ then\\
%\nudge Return 1\\
%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%{
%\hpagess{.49}{.49}
%{
%\experimentv{$\ExpPrivSS{\setprim,\distr{}{},P}{A}$}\\
%$S\getsr \distr{}{}$\\
%$b \getsr \bits$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%$(M_1,\pubaux,\privaux) \getsr \Rep(S)$\\
%$M_0 \getsr P(\pubaux)$\\
%%$(f,v) \getsr \getsr A^{\TestOracle}(M,\pubaux)$\\
%$b' \getsr A^{\TestOracle}(M_b,\pubaux)$\\
%%if $f(S)=v$ then\\
%if $b'=b$ then\\
%\nudge Return 1\\
%Return 0\\
%%
%%\medskip
%%\experimentv{$\ExpPrivSSsim{\setprim,\distr{}{}}{P}$}\\
%%$S\getsr \distr{}{}$\\
%%$(M,\pubaux,\privaux) \getsr \Rep(S)$\\
%%$(f,v) \getsr P(\pubaux)$\\
%%if $f(S)=v$ then\\
%%\nudge Return 1\\
%%Return 0\\
%}
%%
%{
%\oracle{$\TestOracle(q)$}\\
%$a \gets \Qry(M_1,\pubaux,\privaux,q)$\\
%Return~$a$\\
%}
%}
%\caption{Semantic-security style privacy
%  definitions. \textcolor{red}{On the left, need to protect against
%    adversary outputting
%    things like $f(S)=M$.  One idea is to forbid~$f$ such that
%    $f(S)=f(S')$ for all $S,S'$ in the support of $\distr{}{}$.}
%    }
%\label{fig:privacy-ss}
%\end{figure}

\heading{Relationships Among Notions.} \todo{SS$\Rightarrow$OW?
  (Probably not in general, but perhaps for set-multiplicity data
  structures that satisfy certain assumptions on the query set.)
  SS$\not\Rightarrow$ correctness; the converse is already shown by
  the example I gave earlier, i.e. $M=S\setminus\{x\}$ for random $x
  \in S$.}
