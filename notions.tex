
\section{Security Notions}
\label{sec:security-notions}
Following NY, we are interested in defining an adaptive (i.e.,
adversarial) notion of correctness.
In addition to correctness, we establish notions of privacy for
set-multiplicity data structures.
\jnote{I would call it ``soundness'' rather than ``correctness.'' But if you disagree,
just remove this comment and we'll go with correctness.}\tsnote{I
think stick with correctness, since that's the what's used in NY and
in classical literature on BFs.  Or something entirely different, but
I don't know what.  But using ``soundness'' would seem to overload yet
another term, no?}


\fixme{Do we say that all notions can be extended to the ROM by giving
all all algorithms to a RO~$H$?  Is this true for the SS-privacy
notion?  (It seems to make sense elsewhere.)}

\todo{flesh out opening text}

\subsection{Correctness Against Adaptive Adversaries}

First, the adversary $A$ outputs a set $S$ from among those supported by
the data structure. Then,
$\Rep(S)$ is run to generate $(\pubaux,\privaux)$.
%\jnote{I thought we were going to let
%the adversary choose~$S$?} \tsnote{Email discussion. At this point,
%I'm happy just making~$S$ part of the experiment ``name'', the
%way~$\distr{}{}$ is now.  The distribution seems not to matter for
%correctness, at least for the schemes we consider.} \jnote{If instead we choose $S$ from ${\cal D}$, then
%is it clear we want to give $S$ to~$A$?} \tsnote{For \emph{correctness},
%yes, if for no other reason than to allow~$A$ to know when it asking
%a pointless query.  But also it will make no difference to the
%provable correctness of the constructions we consider.}
%\jnote{OK, I think I would just change the definitions so that the attacker
%chooses $S$.}
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
Adversary~$A$ is given~$\pubaux$~as input; it is also
provided with access to an oracle~$\TestOracle$ that, on input a query~$\qry \in \calQ$,
returns $\Qry(\pubaux,\privaux,q)$ and increments
a counter if this result is in error.  The adversary wins if it
manages to cause at least~$r>0$ such errors among its queries.
The notion can be lifted to the random-oracle model by providing
$\Rep,\Qry$ and the adversary~$A$ with oracle access to the
random-oracle(s).

%\jnote{I changed $\TestOracle$ so that it outputs $(a, q(S))$ rather
%than $(a,\mathrm{err})$. Note that the latter can be computed from the former,
%and in the general case I think the former is what we
%want.}\tsnote{This may have implications for the efficiency of
%reductions, I'm not sure. The reason I introduced the $\mathrm{err}$
%counter and returned this was to avoid~$A$ be forced to evaluate
%queries of unknown computational cost.} \jnote{Not sure what you mean. If the
%oracle returns $q(S)$ then $A$ doesn't have to evaluate it. All $A$ has to do
%is an equality check to see if $a=q(S)$ or not.}



\begin{figure}[tp]
\centering
\fpage{.65}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,r}{A}$}\\
$S \getsr A$\\
$\mathcal{C} \gets \emptyset$; $\mathrm{err}\gets 0$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$\perp \getsr A^{\TestOracle}(\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(\qry)$}\\
if $\qry \in \mathcal{C}$ then Return $\bot$\\
$a \gets \Qry(\pubaux,\privaux,\qry)$\\
if $a \neq \qry(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
$\mathcal{C} \gets \mathcal{C} \cup \{\qry\}$ \\
Return~$a$
}
}
\caption{Correctness of~$\setprim$ against an (adaptive) adversary~$A$.}
\label{fig:correctness}
\end{figure}

In this experiment we track the time complexity~$t$ (relative to some
model of computation) and query complexity~$q$ (i.e., number of
queries to $\TestOracle$) of~$A$.
We define the advantage of adversary~$A$ in the correctness experiment as
$\AdvCorrect{\setprim,r}{A} = \Prob{\ExpCorrect{\setprim,r}{A}=1}$.
%\tsnote{drop} Overloading notation, we write $\AdvCorrect{\setprim,r}{t,q}$ for the maximum over
%all~$t$-time adversaries that ask at most~$q$ queries. We say a
%data structure~$\setprim$ is $(t,q,r,\epsilon)$-correct if $\AdvCorrect{\setprim,r}{t,q} \leq \epsilon$.

\todo{[JON] Explain that the case of $r=1$ does not necessarily imply anything for $r>1$: can have a scheme
where finding the first false positive is hard but then the rest are easy; can have
a scheme where finding one false positive is easy but finding more is hard (e.g.,
because there do not exist
any more). Note that the [NY15] definition does not capture this at all.}



\subsection{Privacy}

One can also consider the orthogonal notion of \emph{privacy} for a data structure.
We specialize to the case of set-membership data structures, and
focus on sets rather than multisets. Before continuing, it will be useful to define a notion that captures the difficulty
of guessing an element in a set chosen from some distribution.
Let ${\cal D}$ be a distribution over subsets of some universe ${\cal X}$. The \emph{point-wise
min-entropy} of ${\cal D}$ is defined as
$H_\infty({\cal D}) \bydef -\log \max_{x \in {\cal X}} \Pr[S \getsr {\cal D} : x \in S]$.

\tsnote{I think we should switch the order of presentation, since we
  use the SS notion much more. }
We formalize two types of privacy notions. The first captures
a ``one-wayness'' property that (informally) requires that
the public representation of a set~$S$ does not make it any easier for an attacker
to learn elements of~$S$.  The second definition
captures a flavor of semantic security, essentially requiring that $\pub$ does
not leak any information about the set~$S$ it represents beyond
what is captured by some explicit leakage function.

Our one-wayness notion is defined in Figure~\ref{fig:privacy-ow}.
The relevant experiment begins by first sampling a set $S$ according to a distribution
${\cal D}$. Then $\Rep(S)$ is run to obtain $\pubaux,\privaux$, and the attacker~$A$ is given
$\pubaux$. The attacker succeeds if it is able to output an element of~$S$.
We define the advantage of adversary~$A$ in this experiment\footnote{We
remark that one could consider augmenting experiment $\ExpPrivOW{\setprim,{\cal D}}{A}$ by additionally
giving $A$ access to an oracle for testing membership in~$S$. However, it is not hard
to see that allowing $q$ such queries can increase $A$'s advantage by at most a factor of~$q$.} as
$\AdvPrivOW{\setprim,\mathcal{D}}{A} = \Prob{\ExpPrivOW{\setprim,{\cal D}}{A}=1}$ and,
overloading notation, we write $\AdvPrivOW{\setprim,{\cal D}}{t}$ for the maximum of this value over
all~$t$-time adversaries.
We remark that an attacker can trivially achieve advantage
$2^{-H_\infty({\cal D})}$ by outputting the element most likely to be in~$S$.

\begin{figure}[tp]
\centering
\fpage{.25}{
%\hpagess{.5}{.45}
%{
\experimentv{$\ExpPrivOW{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A(\pubaux)$\\
if $z \in S$ then return 1\\
return 0
%}
%{
%\oracle{$\TestOracle(q)$}\\
%Return $\Qry(\pubaux,\privaux,q)$
%}
}
\caption{Experiment for a ``one-way'' style privacy
  definition.}
\label{fig:privacy-ow}
\end{figure}


Our semantic-security definition is based on the simulation paradigm (see Figure~\ref{fig:privacy-ss}).
\jnote{Here it would not be hard to write the definition for general data structures, not
specifically set-multiplicity data structures.}
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs a set $S$  and is given the
public information $\pubaux$ that results from running $\Rep(S)$.
%\footnote{We do not give $A$ an oracle for issuing queries;
%since $A$ knows the set~$S$, such an oracle would be superfluous.
%\jnote{Interestingly,
%if we provided a query oracle in the real world and an oracle that returns the correct
%answer in the simulated world, then the definition seems to imply
%correctness also.}}
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $\pubaux$ output
by a simulator $\Sim$ that is only given ${\sf leak}(S)$ for some (possibly randomized)
function~${\sf leak}$.
For a given function ${\sf leak}$, adversary $A$, and simulator~$\Sim$,
we define the advantage
$\AdvPrivSS{\setprim,{\sf leak}}{A,\Sim} \bydef
\left| \Prob{\ExpPrivSSreal{\setprim}{A}=1} -
\Prob{\ExpPrivSSsim{\setprim,{\sf leak}}{A, \Sim}=1} \right|$.
Informally, a scheme $\setprim$ is secure with respect to some leakage function ${\sf leak}$
if for all efficient $A$ there is a simulator $\Sim$ such that
$\AdvPrivSS{\setprim,{\sf leak}}{A,\Sim}$ is small.
Intuitively, this means that the public portion
$\pubaux$ of the representation of a set~$S$ leaks~${\sf leak}(S)$ and nothing more.
\jnote{The definition
as stated does not really work well in the presence of (programmable) random oracles.}
\jnote{In the current definition there is (intentionally) no bound on the simulator's
running time.}
\todo{Mention why giving a Test oracle to A doesn't seem to help.}
\begin{figure}[tp]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\setprim}{A}$}\\
$S \getsr A$ \\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\setprim, {\sf leak}}{A, \Sim}$}\\
$S \getsr A$\\
$\pubaux \getsr \Sim({\sf leak}(S))$ \\
return $A(\pubaux)$\\
}
}
\caption{Experiments for the ``semantic-security'' privacy
  definition.}
\label{fig:privacy-ss}
\end{figure}

%\begin{figure}[hbtp]
%\centering
%\fpage{.6}{
%\hpagess{.5}{.45}
%{
%\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
%$S \getsr \distr{}{}$ \\
%$(\pubaux,\privaux) \getsr \Rep(S)$\\
%return $A(S,\pubaux)$\\
%}
%%
%{
%\experimentv{$\ExpPrivSSsim{\setprim, {\sf leak}, \distr{}{}}{A, \Sim}$}\\
%$S \getsr \distr{}{}$\\
%$\pubaux \getsr \Sim({\sf leak}(S))$ \\
%return $A(S,\pubaux)$\\
%}
%}
%\caption{Experiments for the ``distibutional semantic-security'' privacy
%  definition.}
%\label{fig:privacy-dss}
%\end{figure}
\todo{A simple theorem that says if $\privaux=\emptystring$ then
  SS-privacy is unachievable for any leakage other than $\leak(S)=S$?
It's at least worth a highlighted remark, I think.  Currently it is a
remark in the classical BF section, but it seems worth ``pulling up''.} \jnote{Yes, agreed.}
\todo{Since we mention what happens with $\privaux=\emptystring$, also
mention what happens with $\pubaux=\emptystring$ :)}
\heading{Relationships between the notions.}
Intuition suggests that semantic security should imply one-wayness. And, indeed, this is
the case---provided one is careful with the technical details.
For a distribution ${\cal D}$
over subsets of some universe ${\cal X}$, define
\[\tilde H_\infty({\cal D} \mid {\sf leak}) \bydef -\log \sum_{r \in R} \Pr_{S \getsr {\cal D}}[{\sf leak}(S)=r] \cdot
\max_{x \in {\cal X}} \Pr_{S \getsr {\cal D}}[x \in S \mid {\sf leak}(S)=r].\]

\jnote{Make theorem statements consistent -- get rid of $(t, \epsilon)$-style.}

\begin{theorem}\todo{rewrite}
Let $\setprim$ be a set-representation data structure that is $(t, \epsilon)$-semantically secure
with respect to leakage ${\sf leak}:2^{\cal X} \rightarrow R$.
Then $\AdvPrivOW{\setprim,\mathcal{D}}{t} \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})} + \epsilon$.
\end{theorem}
\begin{proof}
Fix a distribution ${\cal D}$ and an attacker
$A$ running in time $t$, and let $\delta \bydef \AdvPrivOW{\setprim,\mathcal{D}}{A}$.
Define an attacker $A'$ who behaves as follows: first, it samples a set $S$ according to
distribution~${\cal D}$. Then, given $\pubaux$, it runs $A(\pubaux)$ to obtain an element~$z$.
Finally, it outputs~1 if and only if $z \in S$.

It is immediate that $\Pr[\ExpPrivSSreal{\setprim}{A'}=1] = \delta$. Semantic
security of $\setprim$ implies that there exists a simulator $\Sim$ for which
$\ExpPrivSSsim{\setprim, {\sf leak}}{A', \Sim} \geq \delta - \epsilon$. However,
the only information
that $\Sim$
and $A'$ jointly have about the set~$S$ that was sampled is ${\sf leak}(S)$; thus,
we must have $\ExpPrivSSsim{\setprim, {\sf leak}}{A', \Sim} \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}$.
This implies
$\delta \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}+\epsilon$ as desired.
\end{proof}

%\todo{SS$\Rightarrow$OW?
%  (Probably not in general, but perhaps for set-multiplicity data
%  structures that satisfy certain assumptions on the qnuery set.)
%  SS$\not\Rightarrow$ correctness; the converse is already shown by
%  the example I gave earlier, i.e. $M=S\setminus\{x\}$ for random $x
%  \in S$.} \jnote{Since we can set $\pubaux$ to be the emptystring, I think
%  it is trivial to see that privacy does not imply correctness.}

\heading{Privacy of ``canonical'' data structures. }
Here we prove a result that can be used to show that a large class of canonical
data structures are SS-secure, with respect to leakage of the size of the set.
\jnote{It would not be too hard to generalize this to arbitrary data structures over
arbitrary objects, so long as the number of distinct queries to the PRF made by $\Rep$
depends only on ${\sf leak}$.}
We first define what we mean by ``canonical.''

%\def\bits{\{0,1\}}

\begin{definition}
A data structure $(\Rep, \Qry)$ is {\sf canonical} if
there exist algorithms $\Rep_1, \Rep_2$, and a function
$F: \calK \times \bits^{\ell_{in}} \rightarrow \bits^{\ell_{out}}$
such that the following hold:
\begin{itemize}
\item $\Rep_1$ takes as input a set $S=\{x_1,\ldots,x_n\}$ and outputs distinct values $s_1, \ldots, s_t \in \bits^{\ell_{in}}$,
where $t$ depends only on~$n$.
\item $\Rep_2$ takes as input $n$ and $y_1, \ldots, y_t \in \bits^{\ell_{out}}$, and outputs
$\pubaux$.
\item For any $S$, the distribution on $\pubaux$ output by $\Rep(S)$ is identical to the distribution
on $\pubaux$ computing as follows:
(1) $s_1, \ldots, s_t \getsr \Rep_1(S)$;
(2) $K \getsr \calK$;
(3) $\pubaux \getsr \Rep_2(n, F_{K}(s_1), \ldots,F_{K}(s_t))$.
%\item $K_1,K_2,\ldots,K_p \getsr \calK$ for some fixed number of keys~$p$;
%\item $\pubaux \getsr \Rep_2(n, \{F_{K_j}(s_1),
%\ldots,F_{K_j}(s_t)\}_{j\in[p]})$.
\end{itemize}
We say that $(\Rep_1, \Rep_2, F)$ is the {\sf canonical representation}
of~$\Rep$. \hfill\dqed \tsnote{Seems odd to call it this without explaining how
  this canonical representation handles $\privaux$, too.} \jnote{It
  doesn't matter what priv is.}
\end{definition}
%\jnote{Not sure how general to make the above. Need to verify that it is general enough
%to prove security of all the constructions we care about.}
\tsnote{Point out an example or two of the schemes this
  covers.  Domain-separated PRF BF, for example.  I think it covers
  classical BF, too, if $F_K(s)=s$ for all $K \in \calK$, $\Rep_1$
  simply returns the elements of~$S$, and $\Rep_2$ actually picks hash
  functions and does the hashing. (Although the following theorem does
  not work in this case, of course.) It does not cover Neidermayer because there's no guarantee of distinct $s_1,\ldots,s_t$. }
  \jnote{I say we state the general theorem in Section~3.2 and just
  promise that applications will come in Section~4.}
We next show that any canonical data structure in which~$F$ is a pseudorandom function is
semantically secure with respect to leakage consisting of the size of the set.

\begin{theorem}\todo{Rewrite to match old DS-PRF proof}
Let $\setprim=(\Rep, \Qry)$ be a canonical data structure for which $\Rep$ has canonical representation
$(\Rep_1, \Rep_2, F)$. If $F$ is a $(t, \epsilon)$-pseudorandom
function \tsnote{overloading~$t$}, then $\setprim$ is semantically secure
with respect to leakage function ${\sf leak}(S) = |S|$.
\end{theorem}
\begin{proof}
We define a simulator $\Sim$ as follows. On input $n$, $\Sim$ fixes
a set $S^*=\{e_1, \ldots, e_n\}$. In then computes: (1)~$s_1,
\ldots, s_t \getsr \Rep_1(S^*)$; (2)~$K \getsr \calK$; (3)~$\pubaux
\getsr \Rep_2(|S|, F_K(s_1), \ldots, F_K(s_t))$, and finally returns
$\pubaux$.

\def\hyb{\mbox{{\sc Hyb}}}

For a given set $S$, define distribution $\mbox{{\sc Real}}_S$ as
\[ \{ (\pubaux, \privaux) \getsr \Rep(S) : \pubaux\},\]
and define distribution $\mbox{{\sf Ideal}}_S$ as
\[ \{ \pubaux \getsr \Sim(|S|) : \pubaux \}.\]
Our goal is to show that $\mbox{{\sc Real}}_S$ and $\mbox{{\sf Ideal}}_S$ are computationally
indistinguishable for any set~$S$. \jnote{I'm not quite sure how to elegantly do a proof
like this in the concrete setting.}\tsnote{I think you just follow the
sketch that I typeset for the Domain-Separated PRF scheme. (I'll embed
notes below, too.) The bound will be exactly what is there, shoudl be able to just cut-and-paste,
basically.  All that's needed, I think, is to do resource accounting
for $\Sim$ and the PRF-adversary~$B$.}
To do so, first note that we can express $\mbox{{\sc Real}}_S$ as the outcome of
the following experiment: \tsnote{I'd call this ``Game $G0$''...}
\begin{enumerate}
\item $s_1, \ldots, s_t \getsr \Rep_1(S)$.
\item $K \getsr \bits^n$.
\item $\pubaux \getsr \Rep_2(|S|, F_k(s_1), \ldots, F_k(s_t))$.
\item Return $\pubaux$.
\end{enumerate}

Now, consider a hybrid distribution $\hyb_S$ computed as follows:
\tsnote{...and this ``Game $G1$''...}
\begin{enumerate}
\item $s_1, \ldots, s_t \getsr \Rep_1(S)$.
\item $y_1, \ldots, y_t \leftarrow \bits^{\ell_{out}}$
\item $\pubaux \getsr \Rep_2(|S|, y_1, \ldots, y_t)$.
\item Return $\pubaux$.
\end{enumerate}
It follows immediately from the fact that $F$ is a pseudorandom function (and efficiency
of $\Rep_2$) that $\hyb_S$ is computationally indistinguishable from $\mbox{{\sc Real}}_S$.
\tsnote{...and then it's clear that the difference between the
  probability of $G0 \outputs 1$ and $G1 \outputs 1$ is exactly the
  PRF advantage of~$B$, who simply uses it's oracle to simulate calls
  to $F_K$ prior to running $\Rep_2$...}
Next, consider the following distribution $\hyb'_S$:
\begin{enumerate}
\item $S^* := \{e_1, \ldots, e_n\}$, where $n = |S|$.
\item $s_1, \ldots, s_{t^*} \getsr \Rep_1(S^*)$.
\item $y_1, \ldots, y_{t^*} \leftarrow \bits^{\ell_{out}}$
\item $\pubaux \getsr \Rep_2(|S|, y_1, \ldots, y_{t^*})$.
\item Return $\pubaux$.
\end{enumerate}
We claim that $\hyb'_S$ is identically distributed to $\hyb_S$. The follows immediately from the
observation that the distribution on $t$ (in $\hyb_S$) is identical to the distribution on~$t^*$
(in $\hyb'_S$) by definition
of what it means to be canonical.
\tsnote{...and it is clear that the distribution of everything is the
  same as in Game $G1$, so we are done. }

To complete the proof, we simply note that $\hyb'_S$ and $\mbox{{\sf Ideal}}_S$
are computationally indistinguishable, again relying on pseudorandomness of~$F$.
\end{proof}
\fixme{ ----The following is left only for cut-and-paste value---- }
\begin{theorem}\label{thm:ds-ss}
Fix $k,m,n>0$, and let $\setprim_{\mathrm{ds}}= (\Rep, \Qry)$ be the set-multiplicity data structure described in Figure~\ref{fig:lin-and-ds}.  For any set~$S \in 2^\elts$, let $\leak(S)=|S|$.  There exists a simulator~$\Sim$ such that, for any adversary~$A$
\[
\AdvPrivSS{\setprim_{\mathrm{ds}},\leak}{A,\Sim} \leq  \AdvPRF{F}{B}
\]
where~$B$ is constructed from~$A$.  (Both~$B$ and~$\Sim$ are explicitly constructed in the proof of this theorem.)
\end{theorem}
\begin{proof}[Proof (sketch). ] \todo{Move this sketch to be a proof
    of Jon's meta-result. Downgrade theorem to be a corollary of that
    result, the proof simply being a description of how $\Rep$
    decomposes into $\Rep_1$ and $\Rep_2$.}
Adversary~$B^{f}$ simulates the steps of $\ExpPrivSSreal{\setprim_{\mathrm{ds}}}{A}$, using its oracle to simuate~$\Rep(S)$.  When~$f=F_K$ then the simulation is perfect.  When~$f=\rho$ then~$B$ simulates $\ExpPrivSSreal{\overline{\setprim}_{\mathrm{ds}}}{A}$ where $\overline{\Rep}$ effectively uses~$k$ independent random functions as the hash functions.  But the $\pub$ produced by $\overline{\Rep}$ has a distribution that depends only~$|S|=n$, not the specific elements of~$S$, and the other public parameters~$m,k$.  Thus, a simulator~$\Sim$ given only $\leak(S)=|S|$ can produce a correct~$\pub$ as follows: starting with an all-zeros array~$M$ of length~$m$, uniformly sample (with replacement)~$nk$ integers in~$[m]$ and set the resulting positions of~$M$ to 1.
\end{proof}
\fixme{ ----The above is left only for cut-and-paste value---- }
