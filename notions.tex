\section{Security Notions}
\label{sec:security-notions}
Following Naor and Yogev, we are interested in defining an adaptive (i.e., adversarial) notion of correctness.
In addition to correctness, we also establish two notions of privacy.


\subsection{Correctness Against Adaptive Adversaries}

We first define a notion of (adversarial) correctness for a data structure~$\Pi$. Our definition
considers an adversary $A$ who may have full information about the data~$S$ being stored (and, indeed,
may even choose~$S$) but does not know or control the randomness used by the representation
algorithm~$\Rep$. Nevertheless, $A$ may learn some partial information about that randomness from
two sources: first, $A$ is given the public portion of the representation output
by~$\Rep$; second, we allow $A$ to adaptively query the $\Qry$ algorithm on inputs
of its choice.

In contrast to the definition considered by Naor and Yogev (and in addition
to other differences between our definitions as discussed in the Introduction), our definition is
parameterized by a value~$r$ that allows for a more fine-grained treatment. The goal of the attacker
is to find $r$ (distinct) points on which the generated representation errs.
Implications of this choice are discussed below.

Formally, our definition is specified by an experiment $\ExpCorrect{\setprim,r}{A}$
given in Figure~\ref{fig:correctness}.
Following an initialization step,
the experiment begins by having the attacker $A$ output $S \in \univ$.
Then, $\Rep(S)$ is run to generate a representation $(\pubaux,\privaux)$ for that data.
%\jnote{I thought we were going to let
%the adversary choose~$S$?} \tsnote{Email discussion. At this point,
%I'm happy just making~$S$ part of the experiment ``name'', the
%way~$\distr{}{}$ is now.  The distribution seems not to matter for
%correctness, at least for the schemes we consider.} \jnote{If instead we choose $S$ from ${\cal D}$, then
%is it clear we want to give $S$ to~$A$?} \tsnote{For \emph{correctness},
%yes, if for no other reason than to allow~$A$ to know when it asking
%a pointless query.  But also it will make no difference to the
%provable correctness of the constructions we consider.}
%\jnote{OK, I think I would just change the definitions so that the attacker
%chooses $S$.}
%(Unless stated otherwise, we assume security holds for all distributions and thus for all sets $S$.)
The adversary~$A$ is given~$\pubaux$, and is also
provided with access to an oracle~$\TestOracle$ that, on input a query~$\qry \in \calQ$,
returns $\Qry(\pubaux,\privaux,\qry)$ to $A$ and also increments
a counter if this result is incorrect (and if $\qry$ is new).  The adversary succeeds if it
manages to cause at least~$r$ such errors among its queries.
The definition can be lifted to the random-oracle model by providing
algorithms $\Rep,\Qry$ with access to the
random oracle, and providing $A$ with such access as well in the
second stage of its execution. (We do not provide $A$ with access to
the random oracle during the first stage when it selects~$S$, as that set
should have no dependence on the random oracle. \jnote{Justify this.})

%\jnote{I changed $\TestOracle$ so that it outputs $(a, q(S))$ rather
%than $(a,\mathrm{err})$. Note that the latter can be computed from the former,
%and in the general case I think the former is what we
%want.}\tsnote{This may have implications for the efficiency of
%reductions, I'm not sure. The reason I introduced the $\mathrm{err}$
%counter and returned this was to avoid~$A$ be forced to evaluate
%queries of unknown computational cost.} \jnote{Not sure what you mean. If the
%oracle returns $q(S)$ then $A$ doesn't have to evaluate it. All $A$ has to do
%is an equality check to see if $a=q(S)$ or not.}



\begin{figure}[bt]
\centering
\fpage{.65}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpCorrect{\setprim,r}{A}$}\\
$\mathcal{C} \gets \emptyset$; $\mathrm{err}\gets 0$\\
$S \getsr A$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$\perp \getsr A^{\TestOracle}(\pubaux)$\\
if $\mathrm{err} < r$ then Return 0\\
Return 1
}
%
{
\oracle{$\TestOracle(\qry)$}\\
if $\qry \in \mathcal{C}$ then Return $\bot$\\
$a \gets \Qry(\pubaux,\privaux,\qry)$\\
if $a \neq \qry(S)$ then \\
\nudge $\mathrm{err}\gets\mathrm{err}+1$\\
$\mathcal{C} \gets \mathcal{C} \cup \{\qry\}$ \\
Return~$a$
}
}
\caption{Correctness of~$\setprim$ against an (adaptive) adversary~$A$.}
\label{fig:correctness}
\end{figure}


In the experiment we track the time complexity~$t$ (relative to some
model of computation) and query complexity~$q$ (i.e., number of
queries to $\TestOracle$) of~$A$.
We define the advantage of adversary~$A$ in the experiment as
$\AdvCorrect{\setprim,r}{A} = \Prob{\ExpCorrect{\setprim,r}{A}=1}$.
We write $\AdvCorrect{\setprim,r}{t,q}$ for the maximum of this value, taken over
all~$t$-time adversaries that ask at most~$q$ queries.
%We say a
%data structure~$\setprim$ is $(t,q,r,\epsilon)$-correct if $\AdvCorrect{\setprim,r}{t,q} \leq \epsilon$.

\heading{Dependence on $r$. }
We observe that there is no automatic relation between
values of $\AdvCorrect{\setprim,r}{t,q}$ for different values of~$r$ (beyond
the trivial observation that the advantage can only possibly decrease as $r$ increases.)
In particular, for any $r>1$ it is not hard to construct a data structure
for which
\[0 < \AdvCorrect{\setprim,1}{t,q} = \AdvCorrect{\setprim,r}{t,q}.\]
(Consider a data structure in which finding the first query $\qry$ that causes an error
is ``hard,'' but for which---having found $\qry$---it is easy to find
many correlated queries that also cause errors.)
At the other extreme, for any $r$ there also exists a data structure
with $\AdvCorrect{\setprim,r}{t,r}=1$ but  $\AdvCorrect{\setprim,r+1}{t,\infty}=0$;
any data structure that always ensures that there are exactly $r$ (easy to find)
queries that cause an error will~do. %\jnote{Not sure if it is worthwhile being more formal.}
We remark that the Naor-Yogev definition does not capture such distinctions
at all.

In Appendix~\ref{sec:compare-defs} we provide further comparison between our
definition and a variant of the Naor-Yogev definition adapted to our syntax.

\def\bool{\{0,1\}}

%\begin{theorem}
%For any $r$, there is a data structure $\setprim$ for which
%\[0 < \AdvCorrect{\setprim,1}{t,q} = \AdvCorrect{\setprim,r}{t,q}.\]
%\end{theorem}
%\begin{proof}
%Let $\Pi'=(\Rep', \Qry')$ be a data structure supporting boolean
%queries $\{\qry : \univ \rightarrow \bool\}$
%over universe~$\univ$, and for which $\AdvCorrect{\setprim',1}{t,q} > 0$.
%\end{proof}


%\tsnote{drop} Overloading notation, we write $\AdvCorrect{\setprim,r}{t,q}$ for the maximum over
%all~$t$-time adversaries that ask at most~$q$ queries. We say a
%data structure~$\setprim$ is $(t,q,r,\epsilon)$-correct if $\AdvCorrect{\setprim,r}{t,q} \leq \epsilon$.




\subsection{Privacy}

We also initiate a consideration of the orthogonal notion of \emph{privacy} for a data structure.
We introduce two notions of privacy. The first is a simulation-based, ``semantic-security''-style
definition that is, in some sense, the strongest notion of privacy one could hope for. The second
is a weaker definition of ``one-wayness,'' specialized to the case
of set-membership data structures, which requires that the public representation data
of some set should not allow an attacker to deduce any element of that set
(beyond what the attacker could have done without the public representation).
This may suffice for many practical applications.

\heading{A simulation-based definition. }
\jnote{I like ``simulation-based security'' rather than ``semantic security.''}
Our simulation-based definition is given in
Figure~\ref{fig:privacy-ss}.
In this definition we compare the probabilities with which an attacker $A$ outputs~1
in two experiments: one ``real'' and one ``simulated.''
In the real experiment (ss-rep-1), $A$ outputs data $S$  and is given the
public information $\pubaux$ that results from running $\Rep(S)$.
%\footnote{We do not give $A$ an oracle for issuing queries;
%since $A$ knows the set~$S$, such an oracle would be superfluous.
%\jnote{Interestingly,
%if we provided a query oracle in the real world and an oracle that returns the correct
%answer in the simulated world, then the definition seems to imply
%correctness also.}}
In the simulated experiment (ss-rep-0), $A$ outputs $S$ and is given $\pubaux$ output
by a simulator $\Sim$ that is only given ${\sf leak}(S)$ for a (possibly randomized)
function~${\sf leak}$. (This leakage function is meant to capture what is leaked
by the public representation in the real world.)
For a given function ${\sf leak}$ and adversary $A$,
define
\begin{eqnarray*}
\lefteqn{\AdvPrivSS{\setprim,{\sf leak}}{A}} \\
& \bydef & \min_{\Sim} \left\{
\left| \Prob{\ExpPrivSSreal{\setprim}{A}=1} -
\Prob{\ExpPrivSSsim{\setprim,{\sf leak}}{A, \Sim}=1} \right|\right\}.\end{eqnarray*}
The definition can be lifted to the random-oracle model by providing
all algorithms with access to the
random oracle. \jnote{Including the simulator? So is this a non-programmable RO? We should comment in any case.}
Informally, a scheme $\setprim$ is secure with respect to some leakage function ${\sf leak}$
if for all efficient $A$ there is a simulator $\Sim$ such that
$\AdvPrivSS{\setprim,{\sf leak}}{A}$ is small.
Intuitively, this means that the public portion
$\pubaux$ of the representation of~$S$ leaks~${\sf leak}(S)$ and nothing more.

\begin{figure}[tb]
\centering
\fpage{.6}{
\hpagess{.5}{.45}
{
\experimentv{$\ExpPrivSSreal{\setprim}{A}$}\\
$S \getsr A$ \\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
return $A(\pubaux)$\\
}
%
{
\experimentv{$\ExpPrivSSsim{\setprim, {\sf leak}}{A, \Sim}$}\\
$S \getsr A$\\
$\pubaux \getsr \Sim({\sf leak}(S))$ \\
return $A(\pubaux)$\\
}
}
\caption{Experiments for a simulation-based privacy
  definition.}
\label{fig:privacy-ss}
\end{figure}


\heading{Remarks on the definition. }
For completeness, we include here a few observations about the definition.
First, our definition does not place any limits on the simulator $\Sim$, and indeed
the simulator may be unbounded. This is only potentially a concern in settings
where $\leak(S)$ allows reconstruction of (information about) $S$ in an information-theoretic
sense, but not in a computationally efficient fashion. Since this setting does not arise
in this paper, allowing unbounded-time simulators is suitable for our purposes. Nevertheless,
in some applications (and in an asymptotic-security treatment) one may also want to limit consideration
to simulators running in some bounded time.

Second, we observe that our definition could be extended by also giving $A$ access to a $\Qry$
oracle in the real world (as in the correctness experiment), while possibly modifying the
ideal world as well. But note that
$A$ knows $S$ and so can compute the true answer $\qry(S)$ on its own (i.e., without a $\Qry$ oracle);
thus, the only way the $\Qry$ oracle can possibly help is if $A$ finds queries
that lead to an error.
We have chosen not to provide $A$ with
access to a $\Qry$ oracle so as to cleanly separate considerations
of correctness and privacy.

Finally, it is worth noting that the ``minimal'' leakage we consider in this work
is leakage of the size of the data, i.e., when $S$ is a set then we would
consider ${\sf leak}(S)=|S|$ as minimal leakage. This is by analogy with encryption,
which generally leaks the length of the message being encrypted.


\heading{``One-wayness.'' }
Here we specialize to the case of set-membership data structures.
Our one-wayness notion is defined in Figure~\ref{fig:privacy-ow}.
The relevant experiment begins by first sampling a set $S$ according to a distribution
${\cal D}$. Then $\Rep(S)$ is run to obtain $\pubaux,\privaux$, and the attacker~$A$ is given
$\pubaux$. The attacker succeeds if it is able to output an element of~$S$.
We define the advantage of adversary~$A$ in this experiment
%\footnote{We
%remark that one could consider augmenting experiment $\ExpPrivOW{\setprim,{\cal D}}{A}$ by additionally
%giving $A$ access to an oracle for testing membership in~$S$. However, it is not hard
%to see that allowing $q$ such queries can increase $A$'s advantage by at most a factor of~$q$.}
as $\AdvPrivOW{\setprim,\mathcal{D}}{A} = \Prob{\ExpPrivOW{\setprim,{\cal D}}{A}=1}$.
% and,
%overloading notation, we write $\AdvPrivOW{\setprim,{\cal D}}{t}$ for the maximum of this value over
%all~$t$-time adversaries.
The definition can be lifted to the random-oracle model by providing
all algorithms (namely, $\Rep,\Qry$, and the adversary~$A$) with access to the
random oracle. (By analogy with the case of correctness, we do not allow the distribution
$\mathcal{D}$ to depend on the random oracle.)

\begin{figure}[tbh]
\centering
\fpage{.25}{
%\hpagess{.5}{.45}
%{
\experimentv{$\ExpPrivOW{\setprim,\distr{}{}}{A}$}\\
$S \getsr \distr{}{}$\\
$(\pubaux,\privaux) \getsr \Rep(S)$\\
$z \getsr A(\pubaux)$\\
if $z \in S$ then return 1\\
return 0
%}
%{
%\oracle{$\TestOracle(q)$}\\
%Return $\Qry(\pubaux,\privaux,q)$
%}
}
\caption{Experiment for a ``one-wayness''-style privacy
  definition.}
\label{fig:privacy-ow}
\end{figure}


It will be useful to define a notion that captures the difficulty
of guessing an element in a set chosen from some distribution.
Let ${\cal D}$ be a distribution over subsets of a universe ${\cal X}$. The \emph{point-wise
min-entropy} of ${\cal D}$ is defined as
$H_\infty({\cal D}) \bydef -\log \max_{x \in {\cal X}} \Pr[S \getsr {\cal D} : x \in S]$.
An attacker can trivially achieve advantage
$\AdvPrivOW{\setprim,\mathcal{D}}{A}=2^{-H_\infty({\cal D})}$ by outputting the element most likely to be in~$S$,
so an attack is only interesting if it performs better than this.




%\begin{figure}[hbtp]
%\centering
%\fpage{.6}{
%\hpagess{.5}{.45}
%{
%\experimentv{$\ExpPrivSSreal{\setprim,\distr{}{}}{A}$}\\
%$S \getsr \distr{}{}$ \\
%$(\pubaux,\privaux) \getsr \Rep(S)$\\
%return $A(S,\pubaux)$\\
%}
%%
%{
%\experimentv{$\ExpPrivSSsim{\setprim, {\sf leak}, \distr{}{}}{A, \Sim}$}\\
%$S \getsr \distr{}{}$\\
%$\pubaux \getsr \Sim({\sf leak}(S))$ \\
%return $A(S,\pubaux)$\\
%}
%}
%\caption{Experiments for the ``distibutional semantic-security'' privacy
%  definition.}
%\label{fig:privacy-dss}
%\end{figure}
%\todo{A simple theorem that says if $\privaux=\emptystring$ then
 % SS-privacy is unachievable for any leakage other than $\leak(S)=S$?
%It's at least worth a highlighted remark, I think.  Currently it is a
%remark in the classical BF section, but it seems worth ``pulling up''.} \jnote{Yes, agreed.}
%\todo{Since we mention what happens with $\privaux=\emptystring$, also%
%mention what happens with $\pubaux=\emptystring$ :)}

\heading{Relationships between the notions. }
Intuition suggests that our simulation-based definition should imply one-wayness. And, indeed, this is
the case---provided one is careful with the technical details.
For a distribution ${\cal D}$
over subsets of some universe ${\cal X}$, define
\[\tilde H_\infty({\cal D} \mid {\sf leak}) \bydef -\log \sum_{r \in R} \Pr_{S \getsr {\cal D}}[{\sf leak}(S)=r] \cdot
\max_{x \in {\cal X}} \Pr_{S \getsr {\cal D}}[x \in S \mid {\sf leak}(S)=r];\]
this merely extends the notion of pointwise min-entropy to condition on some
observed information ${\sf leak}(S)$ about the set in question.

\begin{theorem}
Let $\setprim$ be a set-membership data structure. For any distribution ${\cal D}$,
attacker~$A$, and leakage function~${\sf leak}$, there is an attacker $A'$ running in about the same time as~$A$ with
\[\AdvPrivOW{\setprim,\mathcal{D}}{A} \leq \AdvPrivSS{\setprim,{\sf
    leak}}{A'} + 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}.\]
%Let $\setprim$ be a set-representation data structure that is $(t, \epsilon)$-semantically secure
%with respect to leakage ${\sf leak}:2^{\cal X} \rightarrow R$.
%Then $\AdvPrivOW{\setprim,\mathcal{D}}{t} \leq 2^{-\tilde H_\infty({\cal D} \mid {\sf leak})} + \epsilon$.
\end{theorem}
\begin{proof}
%Fix a  and an attacker
%$A$ running in time $t$, and let $\delta \bydef \AdvPrivOW{\setprim,\mathcal{D}}{A}$.
Define $A'$ as follows: first, it samples a set $S$ according to~${\cal D}$. Then, given $\pubaux$,
it runs $A(\pubaux)$ to obtain~$z$.
Finally, it outputs~1 if and only if $z \in S$.

It is immediate that $\Pr[\ExpPrivSSreal{\setprim}{A'}=1] = \AdvPrivOW{\setprim,\mathcal{D}}{A}$.
But for any simulator $\Sim$,
the only information
that $\Sim$
and $A'$ jointly have in the ideal world
about the set~$S$ that was sampled is ${\sf leak}(S)$; thus,
\[\Prob{\ExpPrivSSsim{\setprim,{\sf leak}}{A', \Sim}=1} \leq
2^{-\tilde H_\infty({\cal D} \mid {\sf leak})}.\]
The theorem follows. \hfill\qed
\end{proof} \pagebreak[1]



%\todo{SS$\Rightarrow$OW?
%  (Probably not in general, but perhaps for set-multiplicity data
%  structures that satisfy certain assumptions on the qnuery set.)
%  SS$\not\Rightarrow$ correctness; the converse is already shown by
%  the example I gave earlier, i.e. $M=S\setminus\{x\}$ for random $x
%  \in S$.} \jnote{Since we can set $\pubaux$ to be the emptystring, I think
%  it is trivial to see that privacy does not imply correctness.}

\heading{Privacy of ``canonical'' data structures. }
Here we prove a result that will be used in the next section
to show that a large class of
set-membership
data structures satisfy our simulation-based notion of privacy
(with respect to leakage of the size of the set). Specifically, we prove that
all ``canonical'' data structures are also private. We define what we mean by canonical next.

%\jnote{It would not be too hard to generalize this to arbitrary data structures over
%arbitrary objects, so long as the number of distinct queries to the PRF made by $\Rep$
%depends only on ${\sf leak}$.}


%\def\bits{\{0,1\}}

\begin{definition}
Let $\Pi = (\Rep, \Qry)$ be a set-membership data structure.
Algorithms $\Rep_1, \Rep_2$ and functions $\ell\colon\mathbb{N}\to\mathbb{N}$,
and
$F: \calK \times \bits^{\ell_{in}} \rightarrow \bits^{\ell_{out}}$
are a \emph{factoring of $\Pi$} \jnote{Better name?}
if the following hold:
\begin{itemize}
\item $\Rep_1$ takes as input a set $S$ and outputs distinct $s_1, \ldots, s_t \in \bits^{\ell_{in}}$,
where $t=\ell(|S|)$
\item $\Rep_2$ takes as input $|S|$ and $y_1, \ldots, y_t \in \bits^{\ell_{out}}$, and outputs
$\pubaux$.
\item For any $S$, the distribution on $\pubaux$ output by $\Rep(S)$ is identical to the distribution
on $\pubaux$ computed as follows:
(1) $s_1, \ldots, s_t \getsr \Rep_1(S)$;
(2) $K \getsr \calK$;
(3) $\pubaux \getsr \Rep_2(|S|, F_{K}(s_1), \ldots,F_{K}(s_t))$. \hfill\dqed
%\item $K_1,K_2,\ldots,K_p \getsr \calK$ for some fixed number of keys~$p$;
%\item $\pubaux \getsr \Rep_2(n, \{F_{K_j}(s_1),
%\ldots,F_{K_j}(s_t)\}_{j\in[p]})$.
\end{itemize}
%We call $(\Rep_1, \Rep_2, \ell, F)$ the {\sf canonical representation}
%of algorithm~$\Rep$.
\end{definition}



%\jnote{Not sure how general to make the above. Need to verify that it is general enough
%to prove security of all the constructions we care about.}
%\tsnote{Point out an example or two of the schemes this
%  covers.  Domain-separated PRF BF, for example.  I think it covers
%  classical BF, too, if $F_K(s)=s$ for all $K \in \calK$, $\Rep_1$
%  simply returns the elements of~$S$, and $\Rep_2$ actually picks hash
%  functions and does the hashing. (Although the following theorem does
%  not work in this case, of course.) It does not cover Neidermayer because there's no guarantee of distinct $s_1,\ldots,s_t$. }
%  \jnote{I say we state the general theorem in Section~3.2 and just
%  promise that applications will come in Section~4.}

Informally, we say that $\Pi$ is \emph{canonical} if it has a factoring $\Rep_1, \Rep_2, \ell, F$ in which
$F$ is a pseudorandom function.
We next show that any canonical data structure satisfies our simulation-based definition
with respect to leakage consisting of the size of the set.

\begin{theorem}\label{thm:priv-ss}
Let $\setprim=(\Rep, \Qry)$ have
factoring
$(\Rep_1, \Rep_2,\ell, F)$. Then for ${\sf leak}(S) = |S|$ and
any attacker $A$, there is an attacker
$B$ making $\ell(|S|)$ queries for which
\[\AdvPrivSS{\setprim,{\sf leak}}{A} \leq \AdvPRF{F}{B}\,. \]
\end{theorem}
\begin{proof}
Define $\Sim$ as follows. On input $n$, $\Sim$ fixes
$S^*=\{e_1, \ldots, e_n\}$, where the $e_i$ are arbitrary distinct values. It then computes $s_1,
\ldots, s_t \getsr \Rep_1(S^*)$, chooses uniform $y_1, \ldots, y_t \getsr \bool^{\ell_{out}}$,
and returns $\pubaux
\getsr \Rep_2(|S|, y_1, \ldots, y_t)$.

\def\hyb{\mbox{{\sc Hyb}}}

Note that because $\Pi$ is canonical, $\Prob{\ExpPrivSSreal{\setprim}{A}=1}$ is equal to the probability that $A$
outputs~1 in
the following experiment~$G_0$:
\begin{enumerate}
\item $S \getsr A$.
\item $s_1, \ldots, s_t \getsr \Rep_1(S)$.
\item $K \getsr \bits^n$.
\item $\pubaux \getsr \Rep_2(|S|, F_k(s_1), \ldots, F_k(s_t))$.
\item Return $A(\pubaux)$.
\end{enumerate}

Now, consider an experiment $G_1$ in which computation is done as follows:
\begin{enumerate}
\item $S \getsr A$.
\item $s_1, \ldots, s_t \getsr \Rep_1(S)$.
\item $y_1, \ldots, y_t \leftarrow \bits^{\ell_{out}}$
\item $\pubaux \getsr \Rep_2(|S|, y_1, \ldots, y_t)$.
\item Return $A(\pubaux)$.
\end{enumerate}
It is immediate that the difference between the probability that $A$ outputs~1 in
$G_1$ and the probability that it outputs~1 in
$G_0$ is bounded by $\AdvPRF{F}{B}$, where $B$ performs the
obvious simulation.

Next, consider the following experiment $G_2$:
\begin{enumerate}
\item $S \getsr A$.
\item $S^* := \{e_1, \ldots, e_n\}$, where $n = |S|$.
\item $s_1, \ldots, s_{t^*} \getsr \Rep_1(S^*)$.
\item $y_1, \ldots, y_{t^*} \leftarrow \bits^{\ell_{out}}$
\item $\pubaux \getsr \Rep_2(|S|, y_1, \ldots, y_{t^*})$.
\item Return $A(\pubaux)$.
\end{enumerate}
We claim that the probability with which $A$ outputs~1 in $G_2$ is \emph{identical} to the
probability with which it outputs~1 in $G_1$.
The follows immediately from the
observation that the distribution on $t$ (in $G_1$) is identical to the distribution on~$t^*$
(in $G_2$) by definition
of canonical, and it is clear that the joint distribution of everything
else is the same as well.

One can verify that $G_2$ is identical to the following experiment:
\begin{enumerate}
\item $S \getsr A$.
\item $\pubaux \getsr \Sim(|S|)$.
\item Return $A(\pubaux)$.
\end{enumerate}
Moreover, the probability with which $A$ outputs~1 in this experiment is
precisely $\Prob{\ExpPrivSSsim{\setprim,{\sf leak}}{A, \Sim}=1}$.
This completes the proof. \hfill\qed
\end{proof}

%\fixme{ ----The following is left only for cut-and-paste value---- }
%\begin{theorem}\label{thm:ds-ss}
%Fix $k,m,n>0$, and let $\setprim_{\mathrm{ds}}= (\Rep, \Qry)$ be the set-multiplicity data structure described in Figure~\ref{fig:lin-and-ds}.  For any set~$S \in 2^\elts$, let $\leak(S)=|S|$.  There exists a simulator~$\Sim$ such that, for any adversary~$A$
%\[
%\AdvPrivSS{\setprim_{\mathrm{ds}},\leak}{A,\Sim} \leq  \AdvPRF{F}{B}
%\]
%where~$B$ is constructed from~$A$.  (Both~$B$ and~$\Sim$ are explicitly constructed in the proof of this theorem.)
%\end{theorem}
%\begin{proof}[Proof (sketch). ] \todo{Move this sketch to be a proof
%    of Jon's meta-result. Downgrade theorem to be a corollary of that
%    result, the proof simply being a description of how $\Rep$
%    decomposes into $\Rep_1$ and $\Rep_2$.}
%Adversary~$B^{f}$ simulates the steps of $\ExpPrivSSreal{\setprim_{\mathrm{ds}}}{A}$, using its oracle to simuate~$\Rep(S)$.  When~$f=F_K$ then the simulation is perfect.  When~$f=\rho$ then~$B$ simulates $\ExpPrivSSreal{\overline{\setprim}_{\mathrm{ds}}}{A}$ where $\overline{\Rep}$ effectively uses~$k$ independent random functions as the hash functions.  But the $\pub$ produced by $\overline{\Rep}$ has a distribution that depends only~$|S|=n$, not the specific elements of~$S$, and the other public parameters~$m,k$.  Thus, a simulator~$\Sim$ given only $\leak(S)=|S|$ can produce a correct~$\pub$ as follows: starting with an all-zeros array~$M$ of length~$m$, uniformly sample (with replacement)~$nk$ integers in~$[m]$ and set the resulting positions of~$M$ to 1.
%\end{proof}
%\fixme{ ----The above is left only for cut-and-paste value---- }
